{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "af632cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "# Create the FrozenLake environment (default: 4x4 grid, slippery surface)\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=True, map_name=\"8x8\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "be7b2bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.99            # Discount factor\n",
    "theta = 1e-8            # Convergence threshold\n",
    "max_iterations = 1000   # Optional iteration limit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "de5d2de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=0.99, theta=1e-8):\n",
    "    \"\"\"\n",
    "    Perform value iteration to compute the optimal state-value function.\n",
    "    \"\"\"\n",
    "    value_table = np.zeros(env.observation_space.n)\n",
    "\n",
    "    for i in range(max_iterations):\n",
    "        delta = 0\n",
    "\n",
    "        for state in range(env.observation_space.n):\n",
    "            old_value = value_table[state]\n",
    "\n",
    "            action_values = []\n",
    "            for action in range(env.action_space.n):\n",
    "                value = 0\n",
    "                for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                    value += prob * (reward + gamma * value_table[next_state] * (not done))\n",
    "                action_values.append(value)\n",
    "\n",
    "            value_table[state] = max(action_values)\n",
    "            delta = max(delta, abs(old_value - value_table[state]))\n",
    "\n",
    "        if delta < theta:\n",
    "            print(f\"Converged in {i+1} iterations.\")\n",
    "            break\n",
    "\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8d5fd395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy(env, value_table, gamma=0.99):\n",
    "    \"\"\"\n",
    "    Derive the optimal policy from the value function.\n",
    "    \"\"\"\n",
    "    policy = np.zeros(env.observation_space.n, dtype=int)\n",
    "\n",
    "    for state in range(env.observation_space.n):\n",
    "        action_values = []\n",
    "\n",
    "        for action in range(env.action_space.n):\n",
    "            value = 0\n",
    "            for prob, next_state, reward, done in env.unwrapped.P[state][action]:\n",
    "                value += prob * (reward + gamma * value_table[next_state] * (not done))\n",
    "            action_values.append(value)\n",
    "\n",
    "        policy[state] = np.argmax(action_values)\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "979b3610",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converged in 347 iterations.\n"
     ]
    }
   ],
   "source": [
    "# Run value iteration and extract optimal policy\n",
    "optimal_value_table = value_iteration(env, gamma, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0953da31",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policy = extract_policy(env, optimal_value_table, gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d4896b16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Value Function:\n",
      "[[0.41464029 0.42720516 0.44614817 0.46832032 0.49244367 0.51656979\n",
      "  0.53526148 0.54097518]\n",
      " [0.41168636 0.42120777 0.43749567 0.45838851 0.48324009 0.51353174\n",
      "  0.54576783 0.55736838]\n",
      " [0.39675202 0.39384048 0.37549622 0.         0.42167796 0.49381917\n",
      "  0.56121205 0.58585888]\n",
      " [0.36927222 0.35298248 0.30653119 0.20040369 0.30075272 0.\n",
      "  0.56901586 0.62825901]\n",
      " [0.33266384 0.2913753  0.19730914 0.         0.28929024 0.36195179\n",
      "  0.53481943 0.6896973 ]\n",
      " [0.30613619 0.         0.         0.08627638 0.21393258 0.27271393\n",
      "  0.         0.77203551]\n",
      " [0.28888542 0.         0.05769637 0.04751101 0.         0.25052147\n",
      "  0.         0.87776873]\n",
      " [0.28038877 0.20081497 0.12732648 0.         0.23959086 0.48644205\n",
      "  0.7371033  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Display results\n",
    "print(\"\\nOptimal Value Function:\")\n",
    "print(optimal_value_table.reshape((8,8)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "79fc968e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
      "[[3 2 2 2 2 2 2 2]\n",
      " [3 3 3 3 3 2 2 1]\n",
      " [3 3 0 0 2 3 2 1]\n",
      " [3 3 3 1 0 0 2 2]\n",
      " [0 3 0 0 2 1 3 2]\n",
      " [0 0 0 1 3 0 0 2]\n",
      " [0 0 1 0 0 0 0 2]\n",
      " [0 1 0 0 1 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nOptimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
    "print(optimal_policy.reshape((8,8)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7cce2bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # For Colab\n",
    "\n",
    "# def run_policy(env, policy, episodes=3):\n",
    "#     \"\"\"\n",
    "#     Simulates the given policy in the FrozenLake environment.\n",
    "#     Uses text rendering (ANSI) for each step.\n",
    "#     \"\"\"\n",
    "#     # Re-create the environment with rendering enabled\n",
    "#     env = gym.make(\"FrozenLake-v1\", is_slippery=True, render_mode=\"ansi\", map_name=\"8x8\")\n",
    "\n",
    "#     for ep in range(episodes):\n",
    "#         state, info = env.reset()\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "#         steps = 0\n",
    "\n",
    "#         print(f\"\\n--- Episode {ep + 1} ---\")\n",
    "\n",
    "#         while not done:\n",
    "#             action = policy[state]\n",
    "#             next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "#             done = terminated or truncated\n",
    "#             total_reward += reward\n",
    "#             steps += 1\n",
    "\n",
    "#             # Render text output of the environment after each action\n",
    "#             print(env.render())\n",
    "\n",
    "#             state = next_state\n",
    "\n",
    "#         print(f\"Episode finished in {steps} steps with reward: {total_reward}\")\n",
    "\n",
    "\n",
    "# # Simulate the optimal policy\n",
    "# run_policy(env, optimal_policy, episodes=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cb68b5ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Episode 1 ---\n",
      "Episode finished in 100 steps with reward: 0.0\n",
      "\n",
      "--- Episode 2 ---\n",
      "Episode finished in 100 steps with reward: 0.0\n",
      "\n",
      "--- Episode 3 ---\n",
      "Episode finished in 43 steps with reward: 1.0\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def run_policy(env, policy, episodes=3, delay=0.5):\n",
    "    \"\"\"\n",
    "    Simulates the given policy in the FrozenLake environment.\n",
    "    Uses graphical rendering (render_mode='human').\n",
    "    \"\"\"\n",
    "    # Create a new environment with graphical rendering\n",
    "    env = gym.make(\"FrozenLake-v1\", map_name=\"8x8\", is_slippery=True, render_mode='human')\n",
    "\n",
    "    for ep in range(episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        print(f\"\\n--- Episode {ep + 1} ---\")\n",
    "\n",
    "        while not done:\n",
    "            action = policy[state]\n",
    "            state, reward, terminated, truncated, _ = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            # Add delay so you can visually follow each step\n",
    "            time.sleep(delay)\n",
    "\n",
    "        print(f\"Episode finished in {steps} steps with reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "\n",
    "run_policy(env, optimal_policy, episodes=3, delay=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f14b4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
