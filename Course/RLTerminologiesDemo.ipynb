{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a540a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "52f55057",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create and load a simulation environment\n",
    "env = gym.make(\"CartPole-v1\", render_mode=\"human\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a0a1a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment: CartPole\n",
    "# Documentation Link: https://gymnasium.farama.org/environments/classic_control/cart_pole/\n",
    "#\n",
    "# Goal: Is to balance the pole on the cart by moving the cart left or right for a given episode\n",
    "#\n",
    "# Agent: Cart\n",
    "#\n",
    "# Actions: 0 ---- Left\n",
    "#          1 ---- Right\n",
    "#\n",
    "# State: [\"Cart Position\",\"Cart Velocity\",\"Pole Angle\",\"poleVelocity\"]\n",
    "#\n",
    "# Reward: 1 for every step taken such that the pole is balanced successfully.\n",
    "#\n",
    "# Termination Condition:\n",
    "# 1. Pole Angle is greater than +-12 DEGREE\n",
    "# 2. Cart Position is greater than +-2.4\n",
    "# 3. Episode length greater than 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebcdb210",
   "metadata": {},
   "source": [
    "# Common Functions used in Gymnasium Env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7735a2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2b37515b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages/pygame/pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "#How to get the initial state?\n",
    "#To bring the agent to the start of the env\n",
    "\n",
    "observation,info = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "765dbaca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04899946,  0.01961563,  0.00404369, -0.00771572], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation\n",
    "# State: [\"Cart Position\",\"Cart Velocity\",\"Pole Angle\",\"poleVelocity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2c82a05e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32a29a74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.int64(1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to generate random action?\n",
    "action = env.action_space.sample()\n",
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "862c3ab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.04939177,  0.21467936,  0.00388937, -0.29912007], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#How to submit action to the environment\n",
    "\n",
    "observation, reward, terminatedStatus, truncatedStatus, info = env.step(action)\n",
    "\n",
    "#observation --- state (current position of the agent)\n",
    "#reward -------- The outcome achieved by the agent based on the current action (+1 or -1)\n",
    "#terminatedStatus -- Whether termination condition is satisified or not (Binary outcome)\n",
    "# truncatedStatus -- Whether episodes are complete or not\n",
    "#info --- additional relevant info of the environment\n",
    "\n",
    "observation, reward, terminatedStatus, truncatedStatus, info "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e80622",
   "metadata": {},
   "source": [
    "# Run a Single Episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee9a9119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.01756928 -0.17010969 -0.01793873  0.29960847]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.02097147  0.0252633  -0.01194656  0.00132242]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [-0.02046621 -0.1696853  -0.01192011  0.29021224]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.02385991 -0.36463526 -0.00611587  0.579112  ]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.03115262 -0.16942814  0.00546637  0.28450873]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [-0.03454118  0.02561541  0.01115655 -0.00644514]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.03402887  0.2205756   0.01102765 -0.29558727]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.02961736  0.02529819  0.0051159   0.00055311]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.0291114   0.2203464   0.00512696 -0.2905113 ]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [-0.02470447  0.02515172 -0.00068326  0.00378419]\n"
     ]
    }
   ],
   "source": [
    "#initialize the state\n",
    "observation,info = env.reset()\n",
    "\n",
    "for episodeStep in range(10):\n",
    "    #Choose a random action\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    #Supply action to the env\n",
    "    newState,reward,isTerminated,isTruncated,info = env.step(action)\n",
    "\n",
    "    #Print info\n",
    "    print(f\"Episode Step {episodeStep} Given Action {action} I got reward {reward} and next state {newState}\")\n",
    "\n",
    "    #Check for Termination\n",
    "    if isTerminated:\n",
    "        print(\"GAME OVER --- Terminated!!!\")\n",
    "        env.close()\n",
    "        break\n",
    "\n",
    "#Check for Truncation(Episode ended)\n",
    "if isTruncated:\n",
    "    print(\"Episode Over. Total Allowed Steps Done. Agent was able to balance pole successfully :)\")\n",
    "\n",
    "env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed5f4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80c22ff",
   "metadata": {},
   "source": [
    "# Run Multiple Episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24649e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.03702192  0.15751     0.0263678  -0.2640338 ]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.04017212 -0.0379782   0.02108712  0.0368478 ]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.03941255 -0.2333961   0.02182408  0.33610862]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [ 0.03474463 -0.03859142  0.02854625  0.05038697]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [ 0.0339728  -0.23411082  0.02955399  0.35193804]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.02929059 -0.4296403   0.03659275  0.6537918 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 0.02069778 -0.23504648  0.04966859  0.37285233]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [ 0.01599685 -0.040664    0.05712564  0.09623476]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [ 0.01518357 -0.2365562   0.05905033  0.40637955]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [ 0.01045245 -0.04231914  0.06717792  0.13288249]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [ 0.00960606  0.15177944  0.06983557 -0.13787322]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.01264165  0.34583527  0.06707811 -0.40773246]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [ 0.01955836  0.5399451   0.05892346 -0.6785363 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [ 0.03035726  0.34405628  0.04535273 -0.36789945]\n",
      "Episode Step 14 Given Action 0 I got reward 1.0 and next state [ 0.03723839  0.14832021  0.03799474 -0.06126814]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [ 0.04020479 -0.04732532  0.03676938  0.24315614]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [ 0.03925828  0.14725265  0.0416325  -0.0377058 ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [ 0.04220334 -0.04844082  0.04087839  0.26781648]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [ 0.04123452 -0.2441216   0.04623472  0.57310736]\n",
      "Episode Step 19 Given Action 1 I got reward 1.0 and next state [ 0.03635209 -0.04967736  0.05769686  0.29534084]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [ 0.03535854 -0.24557236  0.06360368  0.60564786]\n",
      "Episode Step 21 Given Action 1 I got reward 1.0 and next state [ 0.03044709 -0.05139478  0.07571664  0.3336573 ]\n",
      "Episode Step 22 Given Action 0 I got reward 1.0 and next state [ 0.0294192  -0.24750814  0.08238978  0.6492239 ]\n",
      "Episode Step 23 Given Action 0 I got reward 1.0 and next state [ 0.02446904 -0.44367528  0.09537426  0.9666717 ]\n",
      "Episode Step 24 Given Action 1 I got reward 1.0 and next state [ 0.01559553 -0.2499546   0.11470769  0.7054088 ]\n",
      "Episode Step 25 Given Action 0 I got reward 1.0 and next state [ 0.01059644 -0.4464633   0.12881587  1.0318849 ]\n",
      "Episode Step 26 Given Action 0 I got reward 1.0 and next state [ 0.00166717 -0.6430415   0.14945357  1.3620764 ]\n",
      "Episode Step 27 Given Action 0 I got reward 1.0 and next state [-0.01119366 -0.8396865   0.1766951   1.6975318 ]\n",
      "Episode Step 28 Given Action 0 I got reward 1.0 and next state [-0.02798739 -1.0363513   0.21064574  2.0396144 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.00287586 -0.20457074  0.04890419  0.34103754]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.00696727 -0.01017747  0.05572494  0.06416837]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [-0.00717082 -0.20605226  0.05700831  0.37389913]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.01129187 -0.40193573  0.06448629  0.68399835]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.01933058 -0.20776562  0.07816625  0.41229445]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.02348589 -0.40390357  0.08641215  0.7285605 ]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.03156397 -0.21007569  0.10098336  0.46427706]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.03576548 -0.4064688   0.1102689   0.78700465]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.04389486 -0.21302041  0.12600899  0.5309478 ]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [-0.04815526 -0.40966862  0.13662794  0.86053   ]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [-0.05634864 -0.21664543  0.15383855  0.6137372 ]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [-0.06068154 -0.02396982  0.16611329  0.37318736]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [-0.06116094 -0.22101383  0.17357704  0.7132979 ]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [-0.06558122 -0.02866561  0.187843    0.47988686]\n",
      "Episode Step 14 Given Action 0 I got reward 1.0 and next state [-0.06615453 -0.22587343  0.19744073  0.8253967 ]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [-0.070672   -0.42306834  0.21394867  1.1731136 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.04700185  0.21711949 -0.00317065 -0.30855304]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.05134424  0.02204285 -0.00934171 -0.01687174]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.0517851  -0.17294389 -0.00967914  0.2728492 ]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [ 0.04832622  0.02231482 -0.00422216 -0.02287079]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.04877251  0.21749707 -0.00467957 -0.31688285]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.05312246  0.02244208 -0.01101723 -0.02567937]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [ 0.0535713  -0.17252016 -0.01153082  0.26350725]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.05012089 -0.36747563 -0.00626067  0.552531  ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [ 0.04277138 -0.17226632  0.00478995  0.25788218]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.03932606 -0.36745632  0.00994759  0.55207205]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.03197693 -0.56271654  0.02098903  0.8478725 ]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [ 0.0207226  -0.75811845  0.03794648  1.147081  ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [ 0.00556023 -0.56351197  0.0608881   0.86653507]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [-0.00571001 -0.36926913  0.0782188   0.59360075]\n",
      "Episode Step 14 Given Action 0 I got reward 1.0 and next state [-0.01309539 -0.5653938   0.09009082  0.90986115]\n",
      "Episode Step 15 Given Action 1 I got reward 1.0 and next state [-0.02440327 -0.37159902  0.10828804  0.6467988 ]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [-0.03183525 -0.1781392   0.12122402  0.3900833 ]\n",
      "Episode Step 17 Given Action 1 I got reward 1.0 and next state [-0.03539803  0.01507243  0.12902568  0.13794537]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [-0.03509658 -0.18163879  0.13178459  0.46838793]\n",
      "Episode Step 19 Given Action 1 I got reward 1.0 and next state [-0.03872936  0.01139922  0.14115235  0.21997428]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [-0.03850137 -0.18542859  0.14555183  0.5536408 ]\n",
      "Episode Step 21 Given Action 1 I got reward 1.0 and next state [-0.04220995  0.00738188  0.15662464  0.31012458]\n",
      "Episode Step 22 Given Action 1 I got reward 1.0 and next state [-0.04206231  0.19996592  0.16282715  0.07064411]\n",
      "Episode Step 23 Given Action 1 I got reward 1.0 and next state [-0.03806299  0.39242446  0.16424002 -0.16656528]\n",
      "Episode Step 24 Given Action 0 I got reward 1.0 and next state [-0.0302145   0.19537908  0.16090871  0.1730951 ]\n",
      "Episode Step 25 Given Action 1 I got reward 1.0 and next state [-0.02630692  0.38787612  0.16437063 -0.06481719]\n",
      "Episode Step 26 Given Action 0 I got reward 1.0 and next state [-0.0185494   0.1908258   0.16307427  0.27488202]\n",
      "Episode Step 27 Given Action 1 I got reward 1.0 and next state [-0.01473288  0.38329104  0.16857192  0.03774599]\n",
      "Episode Step 28 Given Action 1 I got reward 1.0 and next state [-0.00706706  0.575645    0.16932684 -0.1973694 ]\n",
      "Episode Step 29 Given Action 0 I got reward 1.0 and next state [0.00444584 0.37855676 0.16537945 0.14357951]\n",
      "Episode Step 30 Given Action 0 I got reward 1.0 and next state [0.01201698 0.18150014 0.16825104 0.4835317 ]\n",
      "Episode Step 31 Given Action 1 I got reward 1.0 and next state [0.01564698 0.37389782 0.17792167 0.24824317]\n",
      "Episode Step 32 Given Action 1 I got reward 1.0 and next state [0.02312494 0.5660918  0.18288654 0.01653674]\n",
      "Episode Step 33 Given Action 0 I got reward 1.0 and next state [0.03444677 0.3688824  0.18321727 0.36088687]\n",
      "Episode Step 34 Given Action 1 I got reward 1.0 and next state [0.04182442 0.5609917  0.190435   0.13111106]\n",
      "Episode Step 35 Given Action 0 I got reward 1.0 and next state [0.05304425 0.3637245  0.19305722 0.47731262]\n",
      "Episode Step 36 Given Action 1 I got reward 1.0 and next state [0.06031874 0.5556717  0.20260349 0.2511477 ]\n",
      "Episode Step 37 Given Action 0 I got reward 1.0 and next state [0.07143217 0.35832003 0.20762643 0.600279  ]\n",
      "Episode Step 38 Given Action 1 I got reward 1.0 and next state [0.07859857 0.5500256  0.21963201 0.37950113]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.03680168  0.21580346 -0.03315353 -0.33160034]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.04111775  0.02116872 -0.03978553 -0.04955402]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [ 0.04154113 -0.17336082 -0.04077661  0.23031554]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [ 0.03807391 -0.3678771  -0.0361703   0.50986236]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [ 0.03071637 -0.56247133 -0.02597305  0.79093105]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.01946694 -0.7572272  -0.01015443  1.0753311 ]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [ 0.0043224  -0.95221347  0.01135219  1.3648101 ]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.01472187 -1.1474757   0.03864839  1.6610222 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [-0.03767139 -0.9528248   0.07186884  1.3806236 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [-0.05672789 -0.75866973  0.09948131  1.111254  ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [-0.07190128 -0.95494765  0.12170638  1.4334146 ]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [-0.09100023 -1.1513427   0.15037468  1.7615213 ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [-0.11402708 -0.9582086   0.18560511  1.5191332 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [-0.13319126 -1.1550257   0.21598777  1.8635435 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [-0.0235914   0.18671186 -0.00437889 -0.30719826]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.01985716  0.38189593 -0.01052286 -0.60125893]\n",
      "Episode Step 2 Given Action 0 I got reward 1.0 and next state [-0.01221924  0.18692276 -0.02254803 -0.31190902]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.00848079 -0.00787083 -0.02878622 -0.02642148]\n",
      "Episode Step 4 Given Action 0 I got reward 1.0 and next state [-0.0086382  -0.20256838 -0.02931464  0.2570419 ]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [-0.01268957 -0.00704044 -0.02417381 -0.04474129]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.01283038  0.18841965 -0.02506863 -0.3449522 ]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [-0.00906199  0.38388908 -0.03196768 -0.6454336 ]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [-0.00138421  0.18922685 -0.04487635 -0.36298633]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.00240033 -0.00522951 -0.05213607 -0.08478467]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [ 0.00229574  0.19059952 -0.05383177 -0.39345014]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.00610773  0.3864424  -0.06170077 -0.7026078 ]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.01383658  0.19222741 -0.07575293 -0.4299682 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [ 0.01768113 -0.00174464 -0.08435229 -0.16209464]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [ 0.01764623  0.19447723 -0.08759418 -0.48015112]\n",
      "Episode Step 15 Given Action 1 I got reward 1.0 and next state [ 0.02153578  0.39071944 -0.0971972  -0.7991056 ]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [ 0.02935017  0.5870308  -0.11317932 -1.1207138 ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [ 0.04109079  0.39356038 -0.1355936  -0.86556864]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [ 0.04896199  0.20051861 -0.15290497 -0.61840624]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [ 0.05297237  0.00782577 -0.1652731  -0.3775184 ]\n",
      "Episode Step 20 Given Action 1 I got reward 1.0 and next state [ 0.05312888  0.20486194 -0.17282346 -0.7174146 ]\n",
      "Episode Step 21 Given Action 0 I got reward 1.0 and next state [ 0.05722612  0.01249935 -0.18717176 -0.48372668]\n",
      "Episode Step 22 Given Action 1 I got reward 1.0 and next state [ 0.05747611  0.20970125 -0.19684629 -0.8290709 ]\n",
      "Episode Step 23 Given Action 1 I got reward 1.0 and next state [ 0.06167013  0.40689084 -0.21342771 -1.176643  ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [-0.04547423  0.16109686 -0.03041392 -0.33780682]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [-0.04225229 -0.0335794  -0.03717005 -0.05486781]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.04292388  0.16205525 -0.03826741 -0.35904258]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.03968278 -0.03250239 -0.04544826 -0.07866763]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.04033282  0.16324063 -0.04702161 -0.38533616]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [-0.03706801  0.3589975  -0.05472834 -0.69246614]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [-0.02988806  0.1646758  -0.06857766 -0.41750214]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.02659455 -0.0294107  -0.07692771 -0.14720273]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [-0.02718276 -0.22335155 -0.07987176  0.12025373]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [-0.03164979 -0.4172437  -0.07746668  0.38670743]\n",
      "Episode Step 10 Given Action 1 I got reward 1.0 and next state [-0.03999466 -0.22111244 -0.06973253  0.07064041]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [-0.04441691 -0.02506364 -0.06831972 -0.24320285]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [-0.04491819  0.17096429 -0.07318378 -0.5566291 ]\n",
      "Episode Step 13 Given Action 0 I got reward 1.0 and next state [-0.0414989  -0.02305805 -0.08431637 -0.2878715 ]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [-0.04196006  0.17315876 -0.09007379 -0.6059108 ]\n",
      "Episode Step 15 Given Action 1 I got reward 1.0 and next state [-0.03849689  0.3694171  -0.10219201 -0.92555076]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [-0.03110855  0.5657597  -0.12070303 -1.2485204 ]\n",
      "Episode Step 17 Given Action 1 I got reward 1.0 and next state [-0.01979335  0.7622044  -0.14567344 -1.576444  ]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [-0.00454926  0.569088   -0.17720231 -1.3325143 ]\n",
      "Episode Step 19 Given Action 1 I got reward 1.0 and next state [ 0.0068325   0.7659455  -0.20385261 -1.6750017 ]\n",
      "Episode Step 20 Given Action 1 I got reward 1.0 and next state [ 0.02215141  0.9627663  -0.23735264 -2.0236382 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [-0.03811195 -0.19451821  0.00654014  0.33887926]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [-0.04200231  0.00051007  0.01331773  0.04826588]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [-0.04199211  0.19543855  0.01428304 -0.24018562]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [-0.03808334  0.00011551  0.00947933  0.05696814]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [-0.03808103  0.19510026  0.01061869 -0.23270898]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [-0.03417903 -0.00017179  0.00596451  0.06330443]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [-0.03418246  0.19486414  0.0072306  -0.22749071]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [-0.03028518 -0.00036039  0.00268079  0.06746422]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [-0.03029239 -0.19552067  0.00403007  0.36099175]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [-0.0342028  -0.39069968  0.01124991  0.6549427 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [-0.04201679 -0.5859764   0.02434876  0.9511467 ]\n",
      "Episode Step 11 Given Action 0 I got reward 1.0 and next state [-0.05373632 -0.7814175   0.0433717   1.2513793 ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [-0.06936467 -0.5868773   0.06839928  0.9725906 ]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [-0.08110222 -0.3927365   0.08785109  0.7021541 ]\n",
      "Episode Step 14 Given Action 0 I got reward 1.0 and next state [-0.08895695 -0.58895916  0.10189418  1.0211481 ]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [-0.10073613 -0.7852802   0.12231714  1.3440074 ]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [-0.11644173 -0.5918907   0.14919728  1.091962  ]\n",
      "Episode Step 17 Given Action 1 I got reward 1.0 and next state [-0.12827955 -0.39901572  0.17103653  0.849565  ]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [-0.13625987 -0.5960056   0.18802783  1.1907784 ]\n",
      "Episode Step 19 Given Action 1 I got reward 1.0 and next state [-0.14817998 -0.4037501   0.21184339  0.96243286]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.00862113  0.21388671  0.02092001 -0.32517385]\n",
      "Episode Step 1 Given Action 0 I got reward 1.0 and next state [ 0.01289887  0.01847323  0.01441653 -0.02596775]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [ 0.01326833  0.21338551  0.01389717 -0.31406745]\n",
      "Episode Step 3 Given Action 1 I got reward 1.0 and next state [ 0.01753604  0.40830675  0.00761583 -0.60233545]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.02570218  0.6033214  -0.00443088 -0.89260983]\n",
      "Episode Step 5 Given Action 0 I got reward 1.0 and next state [ 0.03776861  0.40825978 -0.02228308 -0.60132307]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 0.0459338   0.6036863  -0.03430954 -0.90094066]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [ 0.05800753  0.79925585 -0.05232836 -1.2042074 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [ 0.07399264  0.9950137  -0.07641251 -1.5128198 ]\n",
      "Episode Step 9 Given Action 1 I got reward 1.0 and next state [ 0.09389292  1.1909733  -0.1066689  -1.8283448 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.11771239  0.997183   -0.1432358  -1.5706129 ]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.13765605  1.1936947  -0.17464806 -1.9043292 ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [ 0.16152994  1.3902218  -0.21273464 -2.2457213 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 1 I got reward 1.0 and next state [ 0.03592818  0.19802326  0.04391513 -0.23236437]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [ 0.03988864  0.3924911   0.03926784 -0.51087797]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [ 0.04773846  0.5870385   0.02905028 -0.7909324 ]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [ 0.05947924  0.39153     0.01323163 -0.48925373]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.06730983  0.5864628   0.00344656 -0.7777374 ]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [ 0.07903909  0.7815372  -0.01210819 -1.0693339 ]\n",
      "Episode Step 6 Given Action 0 I got reward 1.0 and next state [ 0.09466983  0.5865775  -0.03349487 -0.7804755 ]\n",
      "Episode Step 7 Given Action 1 I got reward 1.0 and next state [ 0.10640138  0.7821435  -0.04910438 -1.0835056 ]\n",
      "Episode Step 8 Given Action 1 I got reward 1.0 and next state [ 0.12204425  0.9778778  -0.0707745  -1.3911841 ]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.14160182  0.783705   -0.09859817 -1.1214443 ]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.15727592  0.5900045  -0.12102706 -0.8612457 ]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.169076    0.78654826 -0.13825198 -1.1894    ]\n",
      "Episode Step 12 Given Action 1 I got reward 1.0 and next state [ 0.18480696  0.9831643  -0.16203998 -1.5220268 ]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [ 0.20447025  1.1798306  -0.1924805  -1.8605932 ]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [ 0.22806686  1.3764727  -0.22969237 -2.2063498 ]\n",
      "GAME OVER --- Terminated!!!\n",
      "Episode Step 0 Given Action 0 I got reward 1.0 and next state [ 0.01127475 -0.23228544 -0.04728854  0.23739529]\n",
      "Episode Step 1 Given Action 1 I got reward 1.0 and next state [ 0.00662904 -0.03652092 -0.04254064 -0.06982088]\n",
      "Episode Step 2 Given Action 1 I got reward 1.0 and next state [ 0.00589862  0.15918429 -0.04393706 -0.37561628]\n",
      "Episode Step 3 Given Action 0 I got reward 1.0 and next state [ 0.00908231 -0.03528695 -0.05144938 -0.09710429]\n",
      "Episode Step 4 Given Action 1 I got reward 1.0 and next state [ 0.00837657  0.16053319 -0.05339147 -0.4055653 ]\n",
      "Episode Step 5 Given Action 1 I got reward 1.0 and next state [ 0.01158723  0.35637003 -0.06150277 -0.71459156]\n",
      "Episode Step 6 Given Action 1 I got reward 1.0 and next state [ 0.01871463  0.55228704 -0.07579461 -1.0259818 ]\n",
      "Episode Step 7 Given Action 0 I got reward 1.0 and next state [ 0.02976037  0.35825157 -0.09631424 -0.75802654]\n",
      "Episode Step 8 Given Action 0 I got reward 1.0 and next state [ 0.03692541  0.16457951 -0.11147477 -0.49713752]\n",
      "Episode Step 9 Given Action 0 I got reward 1.0 and next state [ 0.040217   -0.02880865 -0.12141752 -0.24156153]\n",
      "Episode Step 10 Given Action 0 I got reward 1.0 and next state [ 0.03964082 -0.22200587 -0.12624876  0.01049171]\n",
      "Episode Step 11 Given Action 1 I got reward 1.0 and next state [ 0.0352007  -0.02532069 -0.12603892 -0.31920663]\n",
      "Episode Step 12 Given Action 0 I got reward 1.0 and next state [ 0.03469429 -0.21844335 -0.13242306 -0.06877793]\n",
      "Episode Step 13 Given Action 1 I got reward 1.0 and next state [ 0.03032542 -0.02169596 -0.13379861 -0.40013388]\n",
      "Episode Step 14 Given Action 1 I got reward 1.0 and next state [ 0.02989151  0.17504503 -0.14180128 -0.73182726]\n",
      "Episode Step 15 Given Action 0 I got reward 1.0 and next state [ 0.03339241 -0.01786236 -0.15643783 -0.4869205 ]\n",
      "Episode Step 16 Given Action 1 I got reward 1.0 and next state [ 0.03303516  0.17908087 -0.16617624 -0.8245333 ]\n",
      "Episode Step 17 Given Action 0 I got reward 1.0 and next state [ 0.03661678 -0.0134258  -0.18266691 -0.58838236]\n",
      "Episode Step 18 Given Action 0 I got reward 1.0 and next state [ 0.03634826 -0.20558332 -0.19443455 -0.35834745]\n",
      "Episode Step 19 Given Action 0 I got reward 1.0 and next state [ 0.03223659 -0.39748657 -0.2016015  -0.13272086]\n",
      "Episode Step 20 Given Action 0 I got reward 1.0 and next state [ 0.02428686 -0.58923584 -0.20425592  0.09020769]\n",
      "Episode Step 21 Given Action 0 I got reward 1.0 and next state [ 0.01250215 -0.7809338  -0.20245177  0.31214255]\n",
      "Episode Step 22 Given Action 1 I got reward 1.0 and next state [-0.00311653 -0.58359    -0.19620892 -0.03694255]\n",
      "Episode Step 23 Given Action 0 I got reward 1.0 and next state [-0.01478833 -0.7754364  -0.19694777  0.18798958]\n",
      "Episode Step 24 Given Action 1 I got reward 1.0 and next state [-0.03029706 -0.57812196 -0.19318798 -0.15978952]\n",
      "Episode Step 25 Given Action 1 I got reward 1.0 and next state [-0.0418595  -0.3808347  -0.19638377 -0.50665987]\n",
      "Episode Step 26 Given Action 1 I got reward 1.0 and next state [-0.04947619 -0.18356661 -0.20651697 -0.8542408 ]\n",
      "Episode Step 27 Given Action 0 I got reward 1.0 and next state [-0.05314752 -0.3753662  -0.22360179 -0.63294005]\n",
      "GAME OVER --- Terminated!!!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "for episodeCount in range(1,11):\n",
    "    \n",
    "    #initialize the state\n",
    "    env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
    "    observation,info = env.reset()\n",
    "\n",
    "    for episodeStep in range(400):\n",
    "        #Choose a random action\n",
    "        action = env.action_space.sample()\n",
    "\n",
    "        #Supply action to the env\n",
    "        newState,reward,isTerminated,isTruncated,info = env.step(action)\n",
    "\n",
    "        #Add small delay and call render to see game in execution\n",
    "        time.sleep(0.02) #20ms delay\n",
    "        env.render()\n",
    "\n",
    "        #Print info\n",
    "        print(f\"Episode Step {episodeStep} Given Action {action} I got reward {reward} and next state {newState}\")\n",
    "\n",
    "        #Check for Termination\n",
    "        if isTerminated:\n",
    "            print(\"GAME OVER --- Terminated!!!\")\n",
    "            env.close()\n",
    "            break\n",
    "\n",
    "    #Check for Truncation(Episode ended)\n",
    "    if isTruncated:\n",
    "        (\"Episode Over. Total Allowed Steps Done. Agent was able to balance pole successfully :)\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eba9df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
