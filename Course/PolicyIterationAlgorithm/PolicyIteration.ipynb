{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9292,
     "status": "ok",
     "timestamp": 1753145298276,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "lRxIXXDhmZQW",
    "outputId": "f9fa83a6-7752-43e5-f344-47ff5fd91e0b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gymnasium in /Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.21.0 in /Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages (from gymnasium) (2.3.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages (from gymnasium) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages (from gymnasium) (4.14.0)\n",
      "Requirement already satisfied: farama-notifications>=0.0.1 in /Users/oysterable/delete/ReinforcementLearning/rlvenv/lib/python3.12/site-packages (from gymnasium) (0.0.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "atzspNE-oLLi"
   },
   "outputs": [],
   "source": [
    "# 1. Policy Evaluation\n",
    "\n",
    "# Goal: Calculate the value of each state (V(s)) under the current policy.\n",
    "\n",
    "# How:\n",
    "# For each state, update its value to be the expected sum of rewards the agent would get by following the current policy starting from that state.\n",
    "# What you do:\n",
    "# For every state, use the current policy to decide the action.\n",
    "# Calculate the expected value (reward + discounted future value) for that action.\n",
    "# Repeat for all states, updating their values until they stop changing significantly (convergence).\n",
    "\n",
    "# 2. Policy Improvement\n",
    "\n",
    "# Goal: Improve the policy by acting greedily with respect to the current value function.\n",
    "# How:\n",
    "# For each state, look at all possible actions and choose the action that yields the highest expected value (according to the value function you just computed).\n",
    "# What you do:\n",
    "# For every state, consider all actions.\n",
    "# Pick the action that gives the highest expected value (reward + discounted value of next state).\n",
    "# Update the policy to always pick this best action in that state.\n",
    "\n",
    "\n",
    "# 3. Repeat Until Policy Stable\n",
    "\n",
    "# If the policy didn't change in the last policy improvement step, you're done!\n",
    "# Otherwise, go back to Policy Evaluation and repeat.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Initialize: policy arbitrarily, V(s) = 0 for all states\n",
    "\n",
    "# Repeat:\n",
    "#     1. Policy Evaluation:\n",
    "#         Repeat:\n",
    "#             For each state s:\n",
    "#                 V(s) = expected return from following policy in state s\n",
    "#         Until V(s) converges\n",
    "\n",
    "#     2. Policy Improvement:\n",
    "#         For each state s:\n",
    "#             policy(s) = argmax_a expected return for taking action a in s\n",
    "\n",
    "# Until policy is stable (no change)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 254,
     "status": "ok",
     "timestamp": 1753145305136,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "s0pprqAQmgQb"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1753145316401,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "FmTeRHE9mmWq"
   },
   "outputs": [],
   "source": [
    "# 1. Create the FrozenLake environment (4x4 version, deterministic for clarity)\n",
    "env = gym.make('FrozenLake-v1', map_name=\"4x4\", is_slippery=False)  # Set is_slippery=False for deterministic behavior\n",
    "\n",
    "n_states = env.observation_space.n   # Number of states (16 for 4x4 grid)\n",
    "n_actions = env.action_space.n       # Number of possible actions (4: Left, Down, Right, Up)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1753145325985,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "XVmEIARTmpLY"
   },
   "outputs": [],
   "source": [
    "# 2. Initialize Policy and Value Function\n",
    "policy = np.zeros(n_states, dtype=int)     # Start with \"all left\" actions\n",
    "V = np.zeros(n_states)                     # Value function for all states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 11,
     "status": "ok",
     "timestamp": 1753145335831,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "Ugj8X3Hymrfs"
   },
   "outputs": [],
   "source": [
    "# 3. Set algorithm hyperparameters\n",
    "theta = 1e-8       # Small threshold for policy evaluation convergence\n",
    "gamma = 0.99       # Discount factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1753145490797,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "HkndO_3bmt7q"
   },
   "outputs": [],
   "source": [
    "def one_step_lookahead(state, V):\n",
    "    \"\"\"\n",
    "    Helper function to calculate action values for a given state.\n",
    "    \"\"\"\n",
    "    action_values = np.zeros(n_actions)\n",
    "    for action in range(n_actions):\n",
    "        for prob, next_state, reward, terminated in env.unwrapped.P[state][action]:\n",
    "            action_values[action] += prob * (reward + gamma * V[next_state])\n",
    "    return action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1753145492140,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "P3J48EA0mwpe",
    "outputId": "47c5f6b7-e7da-4eb9-f9f1-967aa8750ce8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1: Policy Stable = False\n",
      "Iteration 2: Policy Stable = False\n",
      "Iteration 3: Policy Stable = False\n",
      "Iteration 4: Policy Stable = False\n",
      "Iteration 5: Policy Stable = False\n",
      "Iteration 6: Policy Stable = False\n",
      "Iteration 7: Policy Stable = True\n"
     ]
    }
   ],
   "source": [
    "# 4. Policy Iteration Algorithm\n",
    "\n",
    "is_policy_stable = False\n",
    "iteration = 0\n",
    "\n",
    "while not is_policy_stable:\n",
    "    iteration += 1\n",
    "    # --- POLICY EVALUATION ---\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(n_states):\n",
    "            v = V[state]\n",
    "            action = policy[state]\n",
    "            v_new = 0\n",
    "            for prob, next_state, reward, terminated in env.unwrapped.P[state][action]:\n",
    "                v_new += prob * (reward + gamma * V[next_state])\n",
    "            V[state] = v_new\n",
    "            delta = max(delta, abs(v - v_new))\n",
    "        if delta < theta:\n",
    "            break\n",
    "\n",
    "    # --- POLICY IMPROVEMENT ---\n",
    "    is_policy_stable = True\n",
    "    for state in range(n_states):\n",
    "        old_action = policy[state]\n",
    "        action_values = one_step_lookahead(state, V)\n",
    "        best_action = np.argmax(action_values)\n",
    "        policy[state] = best_action\n",
    "        if old_action != best_action:\n",
    "            is_policy_stable = False\n",
    "\n",
    "    print(f\"Iteration {iteration}: Policy Stable = {is_policy_stable}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1753145496769,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "xLcTBGCum7L6",
    "outputId": "fbb64fa5-32a6-4f3d-9042-70f8f2080c96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\n",
      "[[1 2 1 0]\n",
      " [1 0 1 0]\n",
      " [2 1 1 0]\n",
      " [0 2 2 0]]\n",
      "\n",
      "Optimal State Value Function:\n",
      "[[0.95099005 0.96059601 0.970299   0.96059601]\n",
      " [0.96059601 0.         0.9801     0.        ]\n",
      " [0.970299   0.9801     0.99       0.        ]\n",
      " [0.         0.99       1.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "# 5. Show results\n",
    "print(\"\\nOptimal Policy (0=Left, 1=Down, 2=Right, 3=Up):\")\n",
    "print(policy.reshape(4, 4))\n",
    "print(\"\\nOptimal State Value Function:\")\n",
    "print(V.reshape(4, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 52,
     "status": "ok",
     "timestamp": 1753145499279,
     "user": {
      "displayName": "Prashant Nair",
      "userId": "01567311054365266478"
     },
     "user_tz": -330
    },
    "id": "0a2PLmPKnBeL",
    "outputId": "eaaf40b8-3b04-4297-a9d3-85988ec00e86"
   },
   "outputs": [],
   "source": [
    "# 6. Evaluate the learned policy\n",
    "def run_episode(env, policy, render=False):\n",
    "    state, _ = env.reset()\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[state]\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "            \n",
    "    env.close()\n",
    "    \n",
    "    return total_reward, steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2JzmCct7nV0o"
   },
   "outputs": [],
   "source": [
    "# Create FrozenLake\n",
    "env = gym.make(\"FrozenLake-v1\", is_slippery=False, render_mode=\"human\", map_name=\"4x4\")\n",
    "\n",
    "\n",
    "n_episodes = 100\n",
    "total_rewards = []\n",
    "for _ in range(n_episodes):\n",
    "    reward, steps = run_episode(env, policy, render=False)\n",
    "    total_rewards.append(reward)\n",
    "\n",
    "print(f\"\\nAverage reward over {n_episodes} episodes: {np.mean(total_rewards)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMNHJVIUgjwPnekP6eggEC6",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
