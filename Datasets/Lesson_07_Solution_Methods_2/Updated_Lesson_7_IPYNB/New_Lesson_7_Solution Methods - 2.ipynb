{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_Lesson_7_Solution Methods - 2.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"lkBxCKhRdKh2"},"source":["# **Solution Methods - 2**\n","\n"]},{"cell_type":"markdown","metadata":{"id":"LURPwjDJdKh4"},"source":["## **Learning Objectives**"]},{"cell_type":"markdown","metadata":{"id":"MqABOBf5dKh5"},"source":["By the end of this lesson, you will be able to:<br>\n","* Explain Deep Reinforcement Learning\n","* Describle and apply Actor Critic and Advantage Actor Critic algorithms "]},{"cell_type":"markdown","metadata":{"id":"Ny82dBL82ewk"},"source":["<b> Deep Reinforcement Learning </b>\n","* Deep reinforcement learning combines artificial neural networks with a reinforcement learning architecture.\n","* It enables RL algorithm-defined agents to learn the best actions possible in virtual environment in order to attain their goals.\n","\n","<b> `Deep Reinforcement Learning = Reinforcement Learning + Deep Learning `</b>\n","\n","* Reinforcement learning defines the objective.\n","* Deep learning gives the mechanism to solve the solution."]},{"cell_type":"markdown","metadata":{"id":"S63hIZYGjJA8"},"source":["## <b> Deep Learning </b>"]},{"cell_type":"markdown","metadata":{"id":"STsfrXFXlZMQ"},"source":["There are some concepts that need to be covered before diving into deep learning. Those are explained below:\n","\n","### <b> Neuron: </b>\n","An artificial neuron is analogous to biological neurons, where each neuron takes inputs, adds weights to them separately, sums them up, and passes this sum through a transfer function to produce a nonlinear output.\n","\n","![neuron](https://drive.google.com/uc?id=1fv1lWyYDul1msnvbWDn1e4zskSBVEDnh)\n","\n","\n","* Here, in = input.\n","\n","#### <b> Comparison Between Biological and Artifical Neuron</b>:\n","![comparison](https://drive.google.com/file/d/1S6dk7cEEKg8fIS1pnbz_Vik4aC4E9iR8)\n","\n","\n","\n","## <b> Neural Network or Artificial Neural Network:</b>\n","An artificial neural network is a system of programs that is patterned after the working of neurons in the human brain and nervous system.\n","* It mainly consists of three layers:\n","\n","![ann](https://drive.google.com/uc?id=1WShhGo_Mt3-LDfy3OF_ZIvPnhvB8C4lf)\n"]},{"cell_type":"markdown","metadata":{"id":"XibdQxvvwMch"},"source":["### <b> Deep Neural Network </b>\n","When a neural network contains more than one hidden layer, it becomes a Deep Neural Network.\n","\n","![DNN](https://drive.google.com/uc?id=1zzFyyE95fQOXwUk8W3eyEAuJd4hmYKD7)\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"q6_Bg6b791y1"},"source":["<br>\n","<br>\n","\n","### <b>Convolutional Neural Network:</b>\n","CNN is a class of deep learning networks that uses convolution in it’s layers. It is mainly applied in visual imaginary analysis."]},{"cell_type":"markdown","metadata":{"id":"lqECiWfY911V"},"source":["### <b>What Is Convolution?</b>\n","* In mathematics, convolution is an operation on two functions that produces a third function, expressing how the shape of one is modified by the other. \n","* The term **convolution** refers to both the result function and the process of computing it."]},{"cell_type":"markdown","metadata":{"id":"oWyZ_QLq9132"},"source":["### <b> Convolution Operation in CNN </b>\n","\n","1. Consider a 6x6 image convolved with 3x3 filter(s) to give an output of size 4x4\n","\n","![cnn_1](https://drive.google.com/uc?id=1Kbedv5x_z6I-NouTMr6MBlIGAdcrKUyv)\n","\n","2. Shift the filter around the input matrix (commonly known as stride) once a convolved output is achieved\n","\n","![cnn_2](https://drive.google.com/uc?id=1COTPdMrM2g7QEQhbBIOAqJODgtG4tAzC)\n","\n","3. Filter operation in stride 2\n","\n","![cnn_3](https://drive.google.com/uc?id=1n7COSL8nd5oPYCo_7eixUuf9V0FnkfiL)\n","\n","\n","4. The final output after applying filter on all the strides:\n","\n","![cnn_4](https://drive.google.com/uc?id=1LHIlmPYjUGfMB0eptSaYrmtBbzFB6kwS)\n","\n","5. The convolution operation gets repeated for each filter resulting in a feature map.\n","\n","![cnn_5](https://drive.google.com/uc?id=1qQrHZDSxjHoDeBAEGeF5uWkmlXKGLWoY)\n","\n","6. The only aim of the convolution operation is to represent the image or the object with fewer regions, but with significant features which reduce the computational complexity.\n","\n","![cnn_6](https://drive.google.com/uc?id=1AUCB9EJT9ZvWhxEpCcKSUwOtuxHMIY7L)\n","\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ilrPIJju919Z"},"source":["Let's see an example of how CNN works:\n","\n","1. The first hidden layer trains on the input and identifies the edges.\n","2. The second hidden layer gets the identified edges as input and gives a combination of edges as an output.\n","3. The third layer distinguishes different facial features that lead to the image recognition of the input.\n","\n","![cnn_eg](https://drive.google.com/uc?id=1Yqi5g1VwbnmNmwYyB9kNGLNhfjbsVmq4)"]},{"cell_type":"markdown","metadata":{"id":"VDAl_oKdEtOm"},"source":["## <b> Deep Q-Learning (DQL): </b>\n","\n","Deep Q-Learning uses a deep neural network for approximating the values as long as the relative importance is preserved. \n","\n","In this algorithm, the initial state is fed into the neural network as the input and it outputs the Q-value of all possible actions. It is a neural networking that maps input states to (action, Q-value) pairs.\n","\n","In Deep Q-Learning, the learning process uses 2 neural networks having different weights but the same architecture. After every N steps, the weights from the main network are copied to the target network. Using this type of dual neural network setup provides more stability in the learning process and it also helps the algorithm to learn more effectively.\n","\n","Below are some key points for Deep Q-learning:\n","\n","* The act of combining Q learning with a deep neural network is called deep Q-learning.\n","* DQL uses Deep Q-Networks."]},{"cell_type":"markdown","metadata":{"id":"DdfPkdrTEtRP"},"source":["<b> Deep Q-Network (DQN): </b>\n","* Deep neural network that approximates a Q-function is called a Deep Q-Network.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"05E4mdYnEtUS"},"source":["![DQN](https://drive.google.com/uc?id=1zvqnBrFPkrnvCRK4EtaI-dVeuShwFaR7)"]},{"cell_type":"markdown","metadata":{"id":"yO1-Y0NVjA8V"},"source":["<b> Q-Learning vs Deep Q-Learning:</b>\n","\n","The major difference between Q-Learning and Deep Q-Learning is how the Q-table is implemented in them. Rather than using a Q-table for mapping a state-action pair as in case of Q-Learning, the Deep Q-Learning replaces the regular Q-table with a neural network for mapping input states to (action, Q-value) pairs.\n","\n","The below diagram shows the difference between Q-learning and Deep Q-learning:\n","\n","![QM](https://drive.google.com/file/d/1xO6taBtMzS4CNfQpy5b94US4ZX_xF--D)\n","\n","![DNN](https://drive.google.com/file/d/19fu_25XAGBCbavurFgJDhzLJOuecc2Xq)"]},{"cell_type":"markdown","metadata":{"id":"EVzwiK6ADx-v"},"source":["<b> Integration of Q-Learning and Deep Neural Network:</b>\n","\n","Suppose we have an arbitrary deep neural network that accepts states from a given environment as input. \n","\n","* For each given state input, the network outputs estimated Q-values for each action that can be taken <br> from that state. \n","\n","* The objective of this network is to approximate the optimal Q-function and it has to satisfy the Bellman equation.\n","\n","* The loss from the network is calculated by comparing the outputted Q-values to the target Q-values from the the Bellman equation.\n","\n","* After the loss is calculated, the weights within the network are updated via stochastic gradient descent and backpropagation like a <br>typical neural network. \n","\n","* Weights of the networks are updated over and over again until we sufficiently minimize the loss and get an approximate optimal <br> Q-function.\n","\n","* Previously, only the Bellman equation was used to compute and update Q-values in Q-table in order to find the optimal Q-function. Now, with deep Q-learning, the network will make use of the Bellman equation to estimate the Q-values to find the optimal Q-function.\n"]},{"cell_type":"markdown","metadata":{"id":"1kvGaHrLEtr_"},"source":["### <b> Input for DQN: </b>\n","* As we have mentioned before, our inputs are states from an environment.\n","Let's take `FrozenLake` environment as an example.\n"," \n","* In this environment, the agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead the agent to fall into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n"," \n","* The surface is described using a grid like the following:\n"," \n","       SFFF       (S: starting point, safe) \n","       FHFH       (F: frozen surface, safe) \n","       FFFH       (H: hole, fall to your doom) \n","       HFFG       (G: goal, where the frisbee is located)\n"," \n","* We can easily represent the states using the above coordinate system from the grid of the environment and use this as input.\n"," \n","* For a more complex environment like a video game, we’ll use images as our input. These images will be continuous snapshots of a game, which will be preprocessed before feeding into the network.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"3gvXiG2pMno0"},"source":["### <b> Hidden Layers of DQN: </b>\n","Deep-Q networks are purely convolutional layers followed by different activation functions and then followed by a couple of fully connected layers.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"zWpYDHQhEtvB"},"source":["### <b> Output Layer of DQN: </b>\n","* The output layer is a fully connected layer and produces the Q-value for each action that can be taken from the given state that is passed as input.\n"," \n","* Let's suppose in a game, the actions we can take consist of moving left, moving right, jumping, and walking. \n"," \n","* Then, the output layer consists of four nodes, each representing one of the four actions. \n"," \n","* Like deep learning, the output layer is not followed by any activation function as the nodes of the output layer should be in their raw forms.\n"," \n","\n"]},{"cell_type":"markdown","metadata":{"id":"kjlUycnbEtx2"},"source":["### <b> Experience Replay and Replay Memory: </b>\n","\n","* In deep Q-networks, we use a technique called <b> experience replay </b> during training. With experience replay, the agent’s experiences are stored at each time step in a data set called the <b> replay memory. </b>\n","\n","* At time t, the agent's experience e$_t$ is defined below:\n","\n","  $e_t$ = ($s_t$, $a_t$, $r_{t+1}$, $s_{t+1}$)\n","\n","* This tuple contains the state of the environment s$_t$, the action a$_t$ taken from state s$_t$, the reward r$_{t+1}$ given to the agent at time t+1 as a result of the previous state-action pair (s$_t$, a$_t$), and the next state of the environment s$_{t+1}$.\n","\n","* All of the agent's experiences at each time step over all episodes are stored in the replay memory. \n","\n","* Actually, in practice, the size of replay memory is usually set to some finite size limit, N. Therefore, it will only store the last N experiences.\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4wnQDKCLEt0o"},"source":["### <b> Significance of Experience Replay: </b>\n","\n","* Random samples are chosen from replay memory to train the network instead of sequential experiences as they occur in the enviroment.\n","\n","* With sequential experiences, the data will become highly correlated which will overfit the model.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Q53HfjFZEt3j"},"source":["## <b> Combining a Deep Q-Network with Experience Replay: </b>\n","\n","### Setup:\n","\n","* Before the training starts, initialize the replay memory data set <b> M </b> to capacity <b> N </b>.So, the replay memory <b> M </b> holds <b>N </b> total experiences.\n","\n","* Then, initialize the network with random weights.\n","\n","* For each episode, the starting state is initialized.\n"]},{"cell_type":"markdown","metadata":{"id":"eWXGEiihEt8G"},"source":["### Gain Experience:\n","\n","* For each time step t within the episode, we either explore the environment and select a random action or explore-exploit the environment and select the greedy action for the given state that gives the highest Q-value.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"VJlCYEo-Et_Q"},"source":["## <b> Training a Deep Q-Network with Replay Memory</b>:\n","* Once experience is stored in replay memory, random batches of experiences are sampled from the replay memory. \n","\n","* Let's say a single experience sample is taken from replay memory. After that, states are preprocessed (if needed) and fed as input to the network. \n","\n","* This network can be called as policy network, as its objective is to approximate the optimal policy by finding the optimal Q-function.\n","\n","* The input state data forward propagates through the network. The model then outputs an estimated Q-value for each possible action from the given input state.\n","\n","* The loss is then calculated by comparing the Q-value output from the network for the action in the experience tuple we sampled and the corresponding optimal Q-value or target Q-value for the same action.\n","\n","### <b> Training the Policy Network</b>:\n","\n","* We calculate the loss from our policy network between the optimal Q-value and the Q-value that was the output from the network for this state-action pair.\n","\n","* We then perform the gradient descent to update the weights in the network in attempts to minimize the loss. \n","\n","* In this case, minimizing the loss means that we’re aiming to make the policy network output Q-values for each state-action pair that approximates the target Q-values given by the Bellman equation.\n","\n","* Till now, everything we mentioned is for one single time step. We then move on to the next time step in the episode and do this process again and again time after time until we reach the end of the episode. \n","\n","* At that point, we start a new episode and do that over and over again until we reach the maximum number of episodes we have set. We’ll  keep repeating this process until we have sufficiently minimized the loss.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"GEzh9JrHoqxm"},"source":["## <b>Policy Gradient:</b>\n","\n","* Till now we have covered two major RL methods, Model-based and Q-learning. \n","\n","* Model-based RL uses the model and the cost function to find the optimal path. Q-Learning uses Q value to derive the optimal policy. \n","\n","* Now, we will cover the third major RL method, Policy Gradient. It focuses only on the policy."]},{"cell_type":"markdown","metadata":{"id":"zRqpRuAnorRF"},"source":["Policy gradient methods are a type of reinforcement learning techniques that rely upon optimizing parametrized policies with respect to the expected return (long-term cumulative reward) by gradient descent. \n"]},{"cell_type":"markdown","metadata":{"id":"80L2-YuuP9xP"},"source":["### <b> Advantages of Policy Gradient: </b>\n","There are three main advantages:\n","\n","1) Among the policy-based methods, policy gradient has the best convergence.\n","\n","2) Policy gradients are more effective in high dimensional action spaces.\n","\n","3) Policy gradients can learn stochastic policies."]},{"cell_type":"markdown","metadata":{"id":"YKG-Tj7oQxxG"},"source":["### <b> Disadvantages of Policy Gradient: </b>\n","\n","The biggest disadvantage of policy gradient is that it takes a lot of time to converge as it proceeds step by step. <br>It takes longer than other methods to train.\n"]},{"cell_type":"markdown","metadata":{"id":"-12UygneSfRS"},"source":["$$\n","\\begin{aligned}\n","v_{k+1}(s) &= E_{\\pi} \\left[ R_{t+1} + \\gamma v_k (S_{t_1} \\mid S_t = s)\\right] \\\\\n","&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"l3F7LM3ZRiRI"},"source":["### <B> Policy Search in Policy Gradient Method</b>:\n","Policy π has a parameter θ where π outputs a probability distribution of actions.\n","\n","Probability of taking an action in a given state s with parameters (θ):\n"]},{"cell_type":"markdown","metadata":{"id":"7bpWatJoRicU"},"source":["$$\n","\\begin{aligned}\n","π_{θ}(a|s) &= P \\left[ a|s\\right]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"81y8H6lVba8w"},"source":["Remember, policy can be seen as an optimization problem.<br>We must find the best parameters (θ) to maximize a score function J(θ).\n"]},{"cell_type":"markdown","metadata":{"id":"u_6kqKf1RiZe"},"source":["$$\n","\\begin{aligned}\n","J(θ)=  E_{πθ} [\\sum_{} πr]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"CWKp__EoRiW_"},"source":["### <B> There are two steps to achieve that: </b>\n","\n","Step 1: Measure the quality of a π (policy) with a policy score function J(θ)\n","\n","Step 2: Use policy gradient ascent to find the best parameter θ that improves our π.\n","\n","`Note: The main idea here is that J(θ) will tell us how good our π is. Policy gradient ascent will help us to find the best policy parameters to maximize the sample of good actions.`"]},{"cell_type":"markdown","metadata":{"id":"LwU8RyWDfKVI"},"source":["  ### <B> Step 1: Policy Score Function J(θ) </b>\n","To measure how good the policy is, we use a function called the objective function (or Policy Score Function) that calculates the expected reward of policy.\n","\n","<b> There are three methods to decide the optimal policy:</b>\n","\n","1) First, in an episodic environment, we can use the start value. Calculate the mean of the return from the first time step (G1). This is the cumulative discounted reward for the entire episode.\n","<br>\n","<br>\n","\n","$$\n","\\begin{aligned}\n","J(θ)=  E_{π} [G_{1} =  R_{1} + \\gamma R_{2} + \\gamma^2R_{3} + ....] = E_{π}(V(s_{1}))\n","\\end{aligned}\n","$$\n","<br>\n","\n","* <b>$[G_{1} =  R_{1} + \\gamma R_{2} + \\gamma^2R_{3} + ....]$ </b> = Cummulative discounted reward starting from the first state\n","\n","* <b> $V(s_{1})$ </b> = Value of state 1\n","\n","We have to find the policy that maximizes G1 because it will be the optimal policy.\n"]},{"cell_type":"markdown","metadata":{"id":"vxoVah0nfKuk"},"source":["2) In a continuous environment, we can use the average value because we can’t rely on a specific start state. Each state value is now weighted (because some happen more than others) by the probability of the occurrence of the respected state.\n","<br>\n","<br>\n","$$\n","\\begin{aligned}\n","J_{avrv}(θ)=  E_{π}(V(s_{1}) = \\sum d(s)V(s)\n","\\end{aligned}\n","$$\n","\n","\n","* $d(s) = \\frac{N(s)}{\\sum_{s^\\prime}N(s^\\prime)}$\n","\n","* $N(s)$ = Number of occurrences of the state\n","\n","* ${\\sum_{s^\\prime}N(s^\\prime)}$ = Total number of occurrences of all the states\n","\n"]},{"cell_type":"markdown","metadata":{"id":"4GpU_10HfKrU"},"source":["3) Here, we use the average reward per time step. The idea here is that we want to get the most reward per time step.\n","\n","<br>\n","<br>\n","\n","$$\n","\\begin{aligned}\n","J_{avrv}(θ)=  E_{π}(r) = \\sum_s d(s)\\sum_a \\pi \\theta(s,a) {R_s}^a\n","\\end{aligned}\n","$$\n","\n","* $ \\sum_s d(s) $ = Probability in state S\n","\n","* $ \\sum_a \\pi \\theta(s,a) $ = Probability after taking action a in state s under the current policy\n","\n","* $ {R_s}^a$ = Immediate reward"]},{"cell_type":"markdown","metadata":{"id":"hyQ6MZcgfKoT"},"source":["### <b> Step 2: Policy Gradient Ascent </b>\n","\n","In the previous step, we got a policy score function that tells us how good our policy is. Now, we have to find a parameter θ that maximizes this score function. Maximizing the score function means finding the optimal policy.\n","\n","* Gradient ascent is the inverse of gradient descent. Remember that gradient always points at the steepest change.\n","\n","* The idea is to find the gradient to the current policy π that updates the parameters in the direction of the greatest increase and iterates.\n","\n","* $ \\pi\\theta $ = Policy\n","\n","* $ J(\\theta) $ = Score Function\n","\n","* $ ∇_\\theta J(\\theta) $ = Gradient\n","\n","* $  ∇_\\theta J(\\theta) + \\theta → \\theta $ = Update\n"]},{"cell_type":"markdown","metadata":{"id":"2vUqof4F-eGN"},"source":["### Policy Gradient:\n","$$\n","\\begin{aligned}\n","E_{\\pi}[∇_\\theta(log\\pi(s,a,\\theta))J(\\theta)]\n","\\end{aligned}\n","$$\n","\n","* $\\pi(a,s,\\theta)$ = Policy Function"]},{"cell_type":"markdown","metadata":{"id":"30S7opLq-eXt"},"source":["### Update Rule:\n","\n","$$\n","\\begin{aligned}\n","∇_\\theta = α*∇_θ(log\\pi(s,a,\\theta))J(\\theta)\n","\\end{aligned}\n","$$\n","\n","\n","* $∇_\\theta$ = Change in Prameters \n","* $ α $ = Learning Rate"]},{"cell_type":"markdown","metadata":{"id":"yTGPMkb5-emg"},"source":["## <b> Actor Critic Method</b>:\n","In Actor critic method instead of waiting until the end of the episode as Monte Carlo, the update is made at each step (TD method).\n","\n","* As the update is done at every step, the total rewards R(t) is not usable.\n","*  This value function replaces the reward function in policy gradient that calculates the rewards only at the end of the episode."]},{"cell_type":"markdown","metadata":{"id":"0vs_ykN2-e0n"},"source":["Policy Update:\n","$$\n","\\begin{aligned}\n","∇_\\theta = α*∇_θ(log\\pi(s,a,\\theta))*J(\\theta)\n","\\end{aligned}\n","$$\n","\n"]},{"cell_type":"markdown","metadata":{"id":"wQtZgtQ0oRxI"},"source":["New way of updating the policy:\n","$$\n","\\begin{aligned}\n","∇_\\theta = α*∇_θ(log\\pi(s,a,\\theta))*Q(S_t,A_t)\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"TS2-T8nYoTA4"},"source":["### <b>Actor</b>: \n","*A* policy function, controls how our agent acts.\n","$$\n","\\begin{aligned}\n","\\pi(s,a,\\theta)\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"6Mp_RGESoZv3"},"source":["### <b>Critic</b>: \n","A value function that measures how good these actions are.\n","$$\n","\\begin{aligned}\n","\\hat{q}(s,a,w)\n","\\end{aligned}\n","$$\n","\n","* $q$ = Weight of the critic that needs to be trained"]},{"cell_type":"markdown","metadata":{"id":"LjvecbeboZ61"},"source":["## <b> How Does Actor Critic Work? </b>\n","\n","* The actor will take random action at the beginning and the critic will provide feedback on that action.\n","* Actor will improve its action based on the feedback provided by the critic and update the policy.\n","* In the meantime, critics will also improve its feedback mechanism.\n","* For both Actor and Critic there will be a neural network, and both run in parallel.\n"]},{"cell_type":"markdown","metadata":{"id":"Ua19SJ6poh5g"},"source":["### <b> The Actor Critic Process</b>:\n","\n","* At each time-step t, we take the current state ($S_t$) from the environment and pass it as an input through our Actor and our Critic.\n","* Policy takes the state, outputs an action ($A_t$), and receives a new state ($A_{t+1}$) and a reward ($S_{t+1}$).\n","\n","* the Critic computes the value of taking that action at that state.\n","* the Actor updates its policy parameters (weights) using this q value.\n","\n","* After updation of parameters, the Actor produces the next action to take at At+1 given the new state St+1. The Critic then updates its value parameters:"]},{"cell_type":"markdown","metadata":{"id":"6t-_WAg4oiKS"},"source":["\n","<b>Policy Update:</b>\n","$$\n","\\begin{aligned}\n","∇_\\theta = α*∇_θ(log\\pi(s,a,\\theta))*\\hat{q}(s,a)\n","\\end{aligned}\n","$$\n","\n","* $ \\hat{q}(s,a) $ = Q-Learning function to esitimate the action value\n","\n","<br>\n","<br>\n","\n","<b>Value Update:</b>\n","\n","$$\n","\\begin{aligned}\n","\\Delta w = \\beta (R(s,a) +γ\\hat{q_w}(s_{t+1}, a_{t+1}) - (s_t, a_t))∇_w\\hat{q}(s_t, a_t)\n","\\end{aligned}\n","$$\n","\n","* $ \\beta $ = Learning rate of value update\n","* $ (R(s,a) +γ\\hat{q_w}(s_{t+1}, a_{t+1}) - (s_t, a_t)) $ = TD error\n","* $ ∇_w\\hat{q}(s_t, a_t) $ = Gradient of the value function"]},{"cell_type":"markdown","metadata":{"id":"_Pxf39QFoiXf"},"source":["### <b>Advantage Actor Critic (A2C):</b>\n","* Actor-Critic agent is mainly implemented by the strategy called Advantage Actor-Critic.\n"," \n","* A2C synchronously updates the global network. It waits until all workers (actors) have finished their training and calculated their gradients to average them, to update our global network.\n"," \n","* A2C waits for each actor to finish their segment of experience before updating the global parameters. Then, we restart a new segment of experience with all parallel actors having the same new parameters.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"whnxZHDAovly"},"source":["![A2C](https://drive.google.com/uc?id=1e2GpI3I2h-6Dq5JUeGRb-Sk4euqv0bf8)"]},{"cell_type":"markdown","metadata":{"id":"TS88ukU2m_s6"},"source":["## <b> Knowledge Check </b>"]},{"cell_type":"markdown","metadata":{"id":"iQzuDGoSm8Pg"},"source":["\n","Click [here](https://www.dropbox.com/s/uodfcnzgw573qe7/Lesson%207%20Knowledge%20Checks.pptx?dl=0) for knowledge check."]},{"cell_type":"markdown","metadata":{"id":"9gch5xfJjwd0"},"source":["## **Key Takeaways**"]},{"cell_type":"markdown","metadata":{"id":"J1h1pvcPjySG"},"source":["* Deep Q-Learning is a combination of deep learning and reinforcement learning.\n","* Policy gradient methods are a type of reinforcement learning techniques that rely upon optimizing parametrized policies with respect to the expected return (long-term cumulative reward) by gradient descent.\n","* In the Actor-Critic method, the actor will take random action at the beginning and the critic will provide feedback on that action."]},{"cell_type":"markdown","metadata":{"id":"JE_6jxZd5wBz"},"source":["![RL 1](https://drive.google.com/uc?id=17MoU7OAf-PPqV3cisfaCVaw6LYtAcrWm)"]},{"cell_type":"code","metadata":{"id":"rKhaKB9SJv9u"},"source":[""],"execution_count":null,"outputs":[]}]}