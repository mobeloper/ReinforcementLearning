{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"New_Lesson 6_ Solution Methods - 1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"KmlaUuxRUwsL"},"source":["# **Solution Methods - 1**"]},{"cell_type":"markdown","metadata":{"id":"_HbHXv0JuxJ8"},"source":["## **Learning Objectives**"]},{"cell_type":"markdown","metadata":{"id":"6I5EqSlcuxUk"},"source":["By the end of this lesson, you will be able to:<br>\n","* Define dynamic programming and use it to solve RL problems \n","* Solve MDP problems using Monte Carlo, Q-learning, and SARSA"]},{"cell_type":"markdown","metadata":{"id":"fjh8ReeThATL"},"source":["## **Methods to Solve the Bellman Equation**\n","\n","\n","    Note: In this lesson, we are going to cover three methods\n","          to solve the Bellman equation, and the rest of the methods are\n","          going to be covered in the next lesson. "]},{"cell_type":"markdown","metadata":{"id":"NKmhzEQK0gg0"},"source":["1. Dynamic programming\n","  * Policy iteration\n","  * Value iteration\n","2. Monte Carlo method\n","3. Temporal difference:\n","  * Q-learning\n","  * SARSA"]},{"cell_type":"markdown","metadata":{"id":"7OoLCHhF7td8"},"source":["## **Dynamic Programming (DP)**"]},{"cell_type":"markdown","metadata":{"id":"EqJFLnfVRxcj"},"source":["### **Understanding Dynamic and Programming**"]},{"cell_type":"markdown","metadata":{"id":"JNNSMPW51H9E"},"source":["#### **Dynamic**\n"]},{"cell_type":"markdown","metadata":{"id":"hQoF1Eup0qWQ"},"source":["It refers to the components in an RL problem that constantly change and<br> require a stepwise solution.<br>\n","\n","These problems possess the following components:\n","* Sequential\n","* Temporal "]},{"cell_type":"markdown","metadata":{"id":"y7ZFJ-bc0tWL"},"source":["#### **Programming**"]},{"cell_type":"markdown","metadata":{"id":"86kve-Wr0qcu"},"source":["It refers to the mathematical programming that optimizes a program.<br>\n","\n","In this setup, a program is denoted as a policy **π**. \n"]},{"cell_type":"markdown","metadata":{"id":"vHs8Eb7Y0qi2"},"source":["### **How Does Dynamic Programming Solve the RL Problems?**\n","\n","    Note: RL problems must have certain properties in order to be solved by DP."]},{"cell_type":"markdown","metadata":{"id":"2DaPs64X0qsH"},"source":["#### **Properties of RL Problems**"]},{"cell_type":"markdown","metadata":{"id":"k_RhfOrD0q_W"},"source":["* Problems must have an **optimal substructure**, which implies that the problems must satisfy the principle of optimality.\n","\n","\n","    Note: The principle of optimality will be discussed in detail in the\n","          topic \"value iteration.\"\n","* Problems must be a collection of subproblems that recur many times. This property allows one to cache and reuse solutions."]},{"cell_type":"markdown","metadata":{"id":"ZJldc_LR0q8m"},"source":["#### **MDP Satisfies the Properties**\n","    Note: The above properties are satisfied by the MDP."]},{"cell_type":"markdown","metadata":{"id":"jkh0R-qD0q6F"},"source":["* Recursive decomposition is provided by the Bellman equation.\n","* Value functions provide caching and reusing of the solutions."]},{"cell_type":"markdown","metadata":{"id":"Qdx8Wjg20q30"},"source":["#### **Solution in a Nutshell**"]},{"cell_type":"markdown","metadata":{"id":"We3-5u3nnVyV"},"source":["The dynamic programming solution includes:\n","* Breaking down the problem into subproblems\n","* Solving all the subproblems\n","* Combining the solutions of all the subproblems"]},{"cell_type":"markdown","metadata":{"id":"3o11L4QI0q0m"},"source":["### **Applications of Dynamic Programming**"]},{"cell_type":"markdown","metadata":{"id":"ygGVh2tM0qqB"},"source":["Apart from solving RL problems, dynamic programming is also used in:\n","* Lattice models in bioinformatics\n","* String algorithms\n","* Graphical models\n","* Graph algorithms"]},{"cell_type":"markdown","metadata":{"id":"MCRtz4jS0qoE"},"source":["### **Planning MDPs**"]},{"cell_type":"markdown","metadata":{"id":"CYWq8-Sgt2yM"},"source":["Here, the intent is to plan MDPs by availing all information on how the<br> environment works."]},{"cell_type":"markdown","metadata":{"id":"TPl8CoYl0qhB"},"source":["#### **Plan MDPs Using Dynamic Programming**"]},{"cell_type":"markdown","metadata":{"id":"VRk3CUW60qa2"},"source":["It is done in two ways:<br>\n","* Solving the prediction problem\n","* Solving the control problem\n","\n","\n","    Note: Before moving to the solutions, let us understand the\n","          gerneral meaning of prediction and control. The concepts of\n","          prediction and control are used by all algorithms covered            \n","          in this chapter. "]},{"cell_type":"markdown","metadata":{"id":"1RQA-6dg0qTv"},"source":["##### **Prediction**"]},{"cell_type":"markdown","metadata":{"id":"QB4WGYoctUc5"},"source":["* In this problem, the MDP (which consists of **S, A, P, R, γ**) or the MRP <br>(which consists of **S, P$^π$, R$^π$, γ**) and policy **π** are already provided.\n","\n","* In order to solve the problem, you must find the value function **v$_π$** that<br> states the amount of reward that can be achieved from any state in the <br>MDP with a given policy."]},{"cell_type":"markdown","metadata":{"id":"6NyCZFrptU_L"},"source":["##### **Control**"]},{"cell_type":"markdown","metadata":{"id":"Uhv6NExgtVEM"},"source":["* In this problem, the MDP (which consists of **S, A, P, R, γ**) is provided<br> but the policy **π** isn't.\n","\n","* In order to solve the problem, you must find the optimal value function<br> **v$_*$** and the optimal policy **π$_∗$** for the given MDP."]},{"cell_type":"markdown","metadata":{"id":"8gepkJVYtVJB"},"source":["### **Dynamic Programming Solutions**"]},{"cell_type":"markdown","metadata":{"id":"x-BI_wE5R7pn"},"source":["#### **Types of DP Solutions**"]},{"cell_type":"markdown","metadata":{"id":"VtwceIaftVHk"},"source":["The following types are used to solve MDP problems:\n","* Policy iteration\n","* Value iteration"]},{"cell_type":"markdown","metadata":{"id":"tn5iRQLytVCq"},"source":["#### **Policy Iteration**"]},{"cell_type":"markdown","metadata":{"id":"2KK_wkKY9VnE"},"source":["This method is used to generate an optimal policy by iteratively finding <br> better estimates of the current policy, at each step, until its convergence.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"rQfaq4rqAaWI"},"source":["In other words, you will start with a random baseline policy **π** and improve <br>it over time until you experience **no significant change in the newly derived <br>policy π$_*$**."]},{"cell_type":"markdown","metadata":{"id":"iM6ZtIO_AOYz"},"source":["##### **Steps in Policy Iteration:**"]},{"cell_type":"markdown","metadata":{"id":"vKWAYT3wr1jU"},"source":["It is composed of two steps:\n","* Policy evaluation (the prediction problem)\n","* Policy improvement (the control problem)"]},{"cell_type":"markdown","metadata":{"id":"jD5RC0cW9M_S"},"source":["##### **Flow of Policy Iteration**"]},{"cell_type":"markdown","metadata":{"id":"iOGb41Q-bY03"},"source":["Let us take a look at an overview of policy iteration:"]},{"cell_type":"markdown","metadata":{"id":"v4KVb3c-9M8s"},"source":["* Evaluate a random policy **π** by finding its state-value functions:\n","  \n","  **v$_π$(s) = E [Rt+1 + γRt+2 + ... $\\mid$ St = s]**\n","\n","* Act greedily with respect to **v$_π$(s)**, in order to generate a better policy **π$'$**:\n","\n","  **π$'$ = greedy(v$_π$)**\n","\n","* Greedy is a systematic optimization of the policy by choosing the next<br> best action that gives the maximum state-action value.\n","\n","    Recall the concept of **greedy** from the lesson **Multi-Arm Bandit**."]},{"cell_type":"markdown","metadata":{"id":"YJvt152O9M6H"},"source":["    Note: The process of policy iteration always converges to an optimal policy π∗."]},{"cell_type":"markdown","metadata":{"id":"PQsSMNDI9M36"},"source":["###### **Diagram of the Process**"]},{"cell_type":"markdown","metadata":{"id":"2u6Nn8uq9M1l"},"source":["![RL L6 1](https://drive.google.com/uc?id=1nA4QIp-MD14Hy84bwV6YM7mh57ZsZWUV)\n","\n","    Source: David Silver"]},{"cell_type":"markdown","metadata":{"id":"n7jPPJNi9MzN"},"source":["###### **Zooming into the Process:**"]},{"cell_type":"markdown","metadata":{"id":"OsfLv_KI9Mwk"},"source":["![RL L6 2](https://drive.google.com/uc?id=1kL0n_-Buq-xnWnxCgx9MZrUs6cY-lIv4)\n","\n","    Source: David Silver"]},{"cell_type":"markdown","metadata":{"id":"-ZRhg76vtU9w"},"source":["##### **Policy Evaluation**"]},{"cell_type":"markdown","metadata":{"id":"_vqLH0g8sS9L"},"source":["As the name suggests, you will evaluate the current policy by calculating its<br> state-value function, that is, the Bellman expectation equation.\n","\n","    Note: In dynamic programming, the Bellman expectation equation\n","          is also referred to as full backups."]},{"cell_type":"markdown","metadata":{"id":"jhrU-c9SIZ5G"},"source":["* **Computing state-value according to the policy:**\n","$$\n","\\begin{aligned}\n","v_\\pi (s) & = E_\\pi \\left[ G_t |S_t = s\\right]\\\\\n","&= E_\\pi \\left[ R_{t+1} + \\gamma G_{t+1} | S_t = s\\right]\\\\\n","&= E_\\pi \\left[ R_{t+1} + \\gamma v_\\pi ( S_{t+1}) \\mid S_t = s \\right]\\\\\n","&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n","\\end{aligned}\n","$$\n","\n","* **Iteratively updating the value of $v_\\pi$ for all $s$:**\n","\n","$$\n","\\begin{aligned}\n","v_{k+1}(s) &= E_{\\pi} \\left[ R_{t+1} + \\gamma v_k (S_{t_1} \\mid S_t = s)\\right] \\\\\n","&= \\sum_a \\pi(a|s) \\sum_{s^\\prime, r} p(s^\\prime, r \\mid s,a) \\left[r + \\gamma v_\\pi (s^\\prime)\\right]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"Lo2hfTDzNAGg"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"_Y5PHgIeNJgR"},"source":["**Problem Statement:** Evaluate the value functions of a 4x4 grid environment.\n","\n","Link of the [dependencies](https://drive.google.com/open?id=1Fsc6IjHPyZazUwDNMzd-vcex4HtChoNB)."]},{"cell_type":"markdown","metadata":{"id":"PamIKK-l9Mrq"},"source":["##### **Policy Improvement**"]},{"cell_type":"markdown","metadata":{"id":"bXoUhOJeLjhY"},"source":["* Policy improvement uses the information collected during policy evaluation<br> of a random policy **π**, that is, state-value function.<br>\n","\n","* It acts greedily on this information to frame a new effective policy **π$'$**, and <br> this is done until the policy converges to get an optimal policy **π$_*$**.\n","\n","\n","    Note: Let us see the mathematical framework."]},{"cell_type":"markdown","metadata":{"id":"SMlV-YmILeXQ"},"source":["* **Computing action-value function:**\n","\n","$$\n","\\begin{aligned}\n","q_\\pi(s,a) &= E \\left[ R_{t+1} + \\gamma v_\\pi (S_{s+1}) \\mid S_t = s, A_t = a \\right] \\\\\n","&= \\sum_{s^\\prime, r} p(s^\\prime, r \\mid, s, a) \\left[ r + \\gamma v_\\pi (s^\\prime) \\right]\n","\\end{aligned}\n","$$\n","\n","* **$\\pi^\\prime$ is a policy for all $s \\in S$:**\n","$$q(s, \\pi^\\prime (s)) \\geq v_\\pi (s)$$\n","\n","  **Where,**\n","$$v_\\pi^\\prime (s) \\geq v_\\pi (s)$$\n","\n","\n","* **Policy improvement:**\n","\n","$$\n","\\begin{aligned}\n","\\pi^\\prime (s) &= argmax_{a} \\space q_\\pi (s,a)\\\\\n","&= argmax_a \\space E \\left[ R_{t+1} + \\gamma v_\\pi (S_{t+1}) \\mid S_t = s, A_t = a \\right] \\\\\n","&= argmax_a \\sum_{s^\\prime, r} \\space p(s^\\prime, s \\mid s, a) \\space [r + \\gamma v_\\pi (s^\\prime)]\n","\\end{aligned}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"I_gwe9hQOWH5"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"IHfTqtqIOZl7"},"source":["**Problem Statement:** Improve the value functions in a 4x4 grid environment to<br> get an optimal policy.\n","\n","Link for the [dependencies](https://drive.google.com/open?id=1Fsc6IjHPyZazUwDNMzd-vcex4HtChoNB)."]},{"cell_type":"markdown","metadata":{"id":"LWEkTJPb9MvG"},"source":["#### **Value Iteration**"]},{"cell_type":"markdown","metadata":{"id":"vR7bZtVj_yNt"},"source":["Value iteration is used to generate an optimal policy by iterative computation<br> of optimal value function of each state until it converges. \n","\n","This method utilizes the concept of **principle of optimality**."]},{"cell_type":"markdown","metadata":{"id":"23nrFIusDZA2"},"source":["**Equation:**<br>\n","**$V_{i+1}$(s) = $\\underset{a}{max}$ $\\underset{s' \\in S}{\\sum}$ $p(s'\\mid s,a)[r + \\gamma V^*(s')]$**<br><br>\n","\n","**Optimal policy:**<br>\n","**$\\pi^*(s)$ = $\\underset{a}{argmax}$ $\\underset{s' \\in S}{\\sum}$ $p(s'\\mid s,a)[r + \\gamma V^*(s')]$**"]},{"cell_type":"markdown","metadata":{"id":"nc8LLZNO9MqO"},"source":["##### **Principle of Optimality**"]},{"cell_type":"markdown","metadata":{"id":"XUFfrKdO9Mlh"},"source":["The process of performing an optimal action (at the start) and then following<br> an optimal policy decides the optimal behavior of an agent."]},{"cell_type":"markdown","metadata":{"id":"5GakvlmmwF4x"},"source":["###### **Components of Optimal Policy**"]},{"cell_type":"markdown","metadata":{"id":"3cAO-A64wdaO"},"source":["The optimal policy can be divided into two parts:\n","* An optimal action **A$^∗$** at the start\n","* An optimal policy from the successor state **S'**"]},{"cell_type":"markdown","metadata":{"id":"PPIuJqkIz9uj"},"source":["###### **What Defines an Optimal Policy**"]},{"cell_type":"markdown","metadata":{"id":"olwOODB_0CCH"},"source":["A policy turns optimal, when:\n","* It is possible to reach a successor state **s'** from the current state **s**.\n","* The policy **$\\pi$** attains optimal value from the successor state **s'**."]},{"cell_type":"markdown","metadata":{"id":"wXQ72xfb9ckc"},"source":["    Note: Let us solve an MDP problem using value iteration. "]},{"cell_type":"markdown","metadata":{"id":"FGxMss3xSt1M"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"XN_QFvvpSw6e"},"source":["**Problem Statement:** Perform value iteration to improve the value functions<br> in a 4x4 grid environment to get an optimal policy.\n","\n","Link for the [dependencies](https://drive.google.com/open?id=1Fsc6IjHPyZazUwDNMzd-vcex4HtChoNB)."]},{"cell_type":"markdown","metadata":{"id":"tv9HvXNg9coE"},"source":["## **Monte Carlo (MC) Method**"]},{"cell_type":"markdown","metadata":{"id":"IV_pgVG692gq"},"source":["### **Understanding the Monte Carlo Method**"]},{"cell_type":"markdown","metadata":{"id":"BF5Kr3fw9ciP"},"source":["* The Monte Carlo method learns from the episodic experience (i.e, sampled experience), <br>where it takes the average of rewards for all the episodes that always terminate. \n","\n","* Monte Carlo is a model-free method that does not require information of MDP<br> during the estimation process. In other words, there is no information of <br>transition probabilities and rewards.\n"]},{"cell_type":"markdown","metadata":{"id":"CZruXRwF5zoN"},"source":["**Equation:**\n","\n","The Monte Carlo method for nonstationary environment can be  written as:\n","\n","$V (S_t) \\leftarrow \\space V (S_t) + \\alpha \\space [G_t \\space V (S_t)]$\n","<br><br>\n","Here:\n","\n","$G_t$ = Actual return following time $t$\n","\n","$\\alpha$ = Constant step-size parameter"]},{"cell_type":"markdown","metadata":{"id":"_u5o1iWBUPhq"},"source":["### **What Makes MC Learning Different from DP?**"]},{"cell_type":"markdown","metadata":{"id":"q1ZqSVcYUXHD"},"source":["* Relies on the simulated interaction with the environment\n","* Attains optimal behavior without having prior knowledge of the dynamics of<br> the environment\n","* Value estimation depends on episodic sense and not on step-by-step sense\n","* Meets the criteria of the law of large numbers and central limit theorem"]},{"cell_type":"markdown","metadata":{"id":"R219t7gams1i"},"source":["### **Intuitively Understanding MC Method**"]},{"cell_type":"markdown","metadata":{"id":"UwYRoD-wm8BQ"},"source":["* The overall problem statement can be stated as: find the scores of the students in the annual exam.\n","\n","* An MC episode can be described as an instance where a student waits till the annual exam to know about the performance. The annual score of a student is denoted as the total return of an episode.\n"]},{"cell_type":"markdown","metadata":{"id":"_TbDGYjpY9CL"},"source":["![RL L6 MC example](https://drive.google.com/uc?id=1DuYZvLTyp0dJqWyx8PtQsFBhz_CgDn81)<br>\n","    "]},{"cell_type":"markdown","metadata":{"id":"GmMymGuJZTml"},"source":["* To solve this we are going to consider multiple episodes, where each episode<br> will have a distinct student and their distinct annual score.\n","\n","    In the end, we will find the **mean score** for all the results."]},{"cell_type":"markdown","metadata":{"id":"MNQDFuTFZqJ8"},"source":["![RL L6 MC example2](https://drive.google.com/uc?id=19dFFZuUmXSwNXNrD3SMB0z9vaV7thKJW)\n","\n","\n","    Note: MC methods uses the emperical mean and not the expectation.\n","          This example holistically explains the MC method and must \n","          not be considered in its literal state."]},{"cell_type":"markdown","metadata":{"id":"1LD0UmRtgglX"},"source":["### **Concepts in MC Method**"]},{"cell_type":"markdown","metadata":{"id":"UUjCX40tgpjw"},"source":["* On-policy prediction and control\n","* Off-policy prediction and control"]},{"cell_type":"markdown","metadata":{"id":"7CTlWjm4H6AN"},"source":["#### **On-policy MC Prediction and Control**"]},{"cell_type":"markdown","metadata":{"id":"qB11QC6kBaf-"},"source":["* **Prediction:** It helps us to find the value function.\n","* **Control:** It helps us to optimize the value function."]},{"cell_type":"markdown","metadata":{"id":"3Kfm8uQbFGd5"},"source":["##### **MC Prediction**"]},{"cell_type":"markdown","metadata":{"id":"ASc40IVCFKSc"},"source":["    Note: In MC method, the reward is derived for each episode.\n","\n","In this step of the MC method:\n","\n","* A random policy is initialized and is used to evaluate the total reward.\n"]},{"cell_type":"markdown","metadata":{"id":"_EsSKKR2MayC"},"source":["* The agent is allowed to work in multiple episodes and perform the following<br> steps for each episode:\n","  * Observe and remember the initial state (i.e., the state at the beginning of <br>the episode) then take an action according to the random policy.\n","  * Agent ventures through all the states that are present in the episode while collecting the reward.\n","  * Calculate the total reward that corresponds to the initial state and initial action<br> of the episode."]},{"cell_type":"markdown","metadata":{"id":"NCL4QtcsMdtL"},"source":["*  Value of Q (s, a) for every state-action pair is: <br>\n","Q (s,a) = Average of R (s, a) originating with (s, a)\n","\n","\n","    Note: The MC method uses emperical mean return instead of expected return.\n","          Check the algorithm below for a better understanding.\n","          Let us solve the prediction problem using MC method with \n","          the help of a demonstration."]},{"cell_type":"markdown","metadata":{"id":"KrOTXXjc6UxK"},"source":["###### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"ipCQAFxK6Zhu"},"source":["**Problem Statement:** Evaluate the functions for a card game \"Blackjack\"."]},{"cell_type":"markdown","metadata":{"id":"cfMwxvoA4EEK"},"source":["##### **MC Control**"]},{"cell_type":"markdown","metadata":{"id":"fjVB28Z64H4x"},"source":["* In this step, the intent is to iteratively get a new policy everytime by acting<br> greedy until the agent arrives at an optimal policy. In other words, the steps <br>of MC prediction are performed in a loop while updating the policy greedily <br>until an optimal policy is derived.\n","\n","\n","    Note: In the MC method, the algorithm uses the epsilon-greedy approach."]},{"cell_type":"markdown","metadata":{"id":"MPb3JPa5nkAU"},"source":["###### **Algorithm with Epsilon-Greedy**"]},{"cell_type":"markdown","metadata":{"id":"E9LAhOo1nrMc"},"source":["![RL L6 5](https://drive.google.com/uc?id=13chyxyvxQqPGGgQ0skvYu9H6gFVlhLXt)\n","\n","    Source: Reinforcement Learning by Richard S. Sutton"]},{"cell_type":"markdown","metadata":{"id":"ZPpzX7j6883F"},"source":["###### **Diagram of the Process**"]},{"cell_type":"markdown","metadata":{"id":"E3f-RbYn8yAJ"},"source":["![RL L6 3](https://drive.google.com/uc?id=13ublB_KN6Xn8Twm0zuRZfTMfyHUUCsy0)\n","\n","    Source: David Silver"]},{"cell_type":"markdown","metadata":{"id":"dnEXkxZ4huXD"},"source":["    Note: Let us understand MC control by performing a demonstration."]},{"cell_type":"markdown","metadata":{"id":"8jxwLCsTMi-s"},"source":["###### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"GDJI0gSLZVKY"},"source":["**Problem Statement:** Train the agent to learn and win the card game \"Blackjack\""]},{"cell_type":"markdown","metadata":{"id":"HaxO7vsxtK07"},"source":["#### **Off-policy MC Prediction and Control**\n","\n","    Note: Like other solutions, this solution also has two \n","          parts: prediction and control."]},{"cell_type":"markdown","metadata":{"id":"WALzOr_dpZbz"},"source":["##### **Prediction**"]},{"cell_type":"markdown","metadata":{"id":"vNVhyu1vpect"},"source":["* The off-policy method is governed by two concepts:\n","  * Behavior policy: It samples all possible actions.\n","  * Target policy: It is the policy that gets evaluated and improved. \n","\n","* The sampling method used here is weighted importance sampling.\n","\n","\n","    Note: Importance sampling can either be ordinary or weighted.\n","          In this section we are using the weighted importance \n","          sampling (which is recommended and widely used in research).\n"," \n","* In off-policy method, the approximation <br>$Q$ converges to $q_\\pi$ for all state-action pairs.\n","\n","* The actions in this method are determined from behavior policy.\n","\n","* This method can be used in on-policy method by taking the same policy <br>as the target and behavior policy under the following condition:\n","  * $\\pi = b$\n","  * $Weight \\space W = 1$"]},{"cell_type":"markdown","metadata":{"id":"ltpk1CrOqz1Z"},"source":["###### **Algorithm for Off-policy Prediction**"]},{"cell_type":"markdown","metadata":{"id":"QajthJZ3q5kG"},"source":["![RL L6 6](https://drive.google.com/uc?id=1AIFqpWrrEYv-6U98ClZ6uF5at2U2IsVi)\n","\n","    W = Vector of weights underlying an approximate value function\n","    C = Cumulative sum of the weights given to the first n returns\n","\n","    \n","    Source: Reinforcement Learning by Richard S. Sutton"]},{"cell_type":"markdown","metadata":{"id":"3y06KaYPylPK"},"source":["##### **Control**"]},{"cell_type":"markdown","metadata":{"id":"plwGy1QatLCZ"},"source":["* The off-policy control method tracks the behavior policy for actions while<br> learning and improving the target policy.\n","\n","* Condition: <br>\n","  * Behavior policy must select all the possible actions (i.e., nonzero probability<br> of selecting all the actions) that can potentially be selected by the target policy.\n","\n","  * The target policy is a greedy policy with respect to Q, whereas the behavior <br>policy $b$ must obtain an infinite number of returns for each state and action pair."]},{"cell_type":"markdown","metadata":{"id":"PJJ598wy4XTp"},"source":["###### **Algorithm for Off-policy Control**"]},{"cell_type":"markdown","metadata":{"id":"Gfiv1Fn_4bnh"},"source":["![RL L6 7](https://drive.google.com/uc?id=1vueVCXoR2G5wWiee_r0IhPBV_VaiIDJT)\n","\n","\n","    Source: Reinforcement Learning by Richard S. Sutton"]},{"cell_type":"markdown","metadata":{"id":"FycI9iT1-rzJ"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"hO7xbsy0_Xb1"},"source":["**Problem Statement:** Train the agent to learn and win the card game \"Blackjack\" using off-policy MC method."]},{"cell_type":"markdown","metadata":{"id":"hNZl0QFKYqOt"},"source":["## **Temporal Difference (TD)**"]},{"cell_type":"markdown","metadata":{"id":"dRsUtfPXGhye"},"source":["It is a reinforcement learning method that learns by sampling the environment and<br> updating the current estimates of the value function."]},{"cell_type":"markdown","metadata":{"id":"_E7MsaG1JelZ"},"source":["### **Features of TD** "]},{"cell_type":"markdown","metadata":{"id":"h8N24B8BYqWu"},"source":["* Does not require prior knowldege of probability transitions and rewards\n","\n","* Learns from episodes of experience, like the MC method\n","\n","* Combines the following features of DP and MC:\n","  * Raw experience of the environment is taken from MC, i.e., model-free method \n","  * Updating the value functions using the learned estimates which is similar to DP"]},{"cell_type":"markdown","metadata":{"id":"pfzNIBBXamzl"},"source":["### **Intuitively Understanding TD**"]},{"cell_type":"markdown","metadata":{"id":"yK9TfWZfanbM"},"source":["* The overall problem statement can be stated as: find how the students of a class scores <br>in the annual exam.\n","\n","* A TD sample can be described as an instance where a student does not wait till the<br> annual exam to access the performance and updates (i.e., at each weekly test.)<br><br>\n","The annual score (i.e., total reward) of a student is denoted as the sum of the return of all weekly tests.\n","\n","\n","    Note: TD can be applied to episodic as well nonepisodic instances.\n"]},{"cell_type":"markdown","metadata":{"id":"zodqK5yxano2"},"source":["![RL L6 TD 1](https://drive.google.com/uc?id=1ASQqhEMBycyw2FaA7TjGGiaCHQD0KOMi)"]},{"cell_type":"markdown","metadata":{"id":"oU8j5iigYqbl"},"source":["### **TD Prediction and Control**"]},{"cell_type":"markdown","metadata":{"id":"vs-tvbVdE8ZF"},"source":["#### **TD Prediction**"]},{"cell_type":"markdown","metadata":{"id":"2ORAOrh5Yqh6"},"source":["TD policy evaluation method estimates the value function $v_\\pi$ for a given policy $\\pi$.\n","\n","\n","Let us understand this using a dummy algorithm for one-step TD:\n","\n","    Note: Remember, this algorithm will look only one-step in the future.\n","\n","\n","![RL L6 8](https://drive.google.com/uc?id=1wJN-6RKpT2zXHu-x7F8rd9ZBIe0z92lx)\n","\n","    Source: Reinforcement Learning by Richard S. Sutton\n","\n","    \n","    Hyperparameters: \n","    Lambda (λ): Credit assignment variable \n","    The value ranges from 0 to 1. \n","    The higher the value, the more credit can be assigned to \n","    back states and actions in the backward view.\n","\n","    Alpha (α): Learning rate \n","    The value ranges from 0 to 1.\n","    It tell us the acceptable error and helps in adjusting \n","    the estimates accordingly. \n","    High values adjust aggressively, i.e., accepting more errors,\n","    whereas small values adjust onservatively.  \n","    "]},{"cell_type":"markdown","metadata":{"id":"grhIA7DEYqmB"},"source":["##### **Target and Error**"]},{"cell_type":"markdown","metadata":{"id":"mFGtnP45YqaC"},"source":["We already studied the derivation of the estimate for $V_\\pi$ in the MDP chapter. <br>\n","\n","Now, let us see which part of the expanded equation is used by MC and DP methods.\n","\n","\n","![RL L6 9](https://drive.google.com/uc?id=1O1_XKl2Dj7YXoP-PHRyamXoSIJoHmQuU)\n","\n","\n","    Note: The target for DP and TD are the same.\n"]},{"cell_type":"markdown","metadata":{"id":"A4HR9jfYYqgB"},"source":["* TD uses the concept of DP to determine the target and error.\n","\n","  * **TD target:**<br> **$\\space r_{t+1} + γV^\\pi(s_{t+1})\\space$**<br>\n","It differs from the MC target update **$G_t$** (i.e., **$R_t$**).  \n","\n","  * **TD error:**<br> $\\space \\delta_t = r_{t+1} + γV^\\pi(s_{t+1}) − V(s_t)$ <br>\n","TD error at **each time** is the error in the estimated value **made at that time**."]},{"cell_type":"markdown","metadata":{"id":"vxWrzNVVV7xw"},"source":["##### **An Overview of TD ($\\lambda$) and Eligibility Traces**"]},{"cell_type":"markdown","metadata":{"id":"geLQI3IbWCTh"},"source":["At first, we are going to discuss why we need **TD ($\\lambda$)**.\n","We need **TD ($\\lambda$)** for:\n","* Updating the values before an episode comes to an end\n","* Deriving estimates using more than one-step look ahead strategy\n","\n","\n","    Note: Let us understand this with the help of an example. \n","          Remember, we are not using this example to teach temporal\n","          difference. Rather, to learn about multi-step look ahead. "]},{"cell_type":"markdown","metadata":{"id":"I2RSu7ExoiKH"},"source":["**Example:**\n","\n","\n","\n","![RL L6 TD lambda](https://drive.google.com/uc?id=1pZlWoLYsX_PnsdsFF4e5_u2ZW4Q3NDui)\n","\n","    Source: Reinforcement Learning by Richard S. Sutton\n","\n","<br><br>\n","**Environment:**\n","\n","The above diagram depicts an environment, where:\n","* Agent always starts at state **D**\n","* Agent is allowed to move randomly toward its left or right with a probability of 50%\n","* **A** and **G** are terminal states\n","* Ending the episode at **A** will fetch a reward of **0**\n","* Ending the episode at **G** will fetch a reward of **1**\n","* No reward is awarded for states **B** through **F**\n","<br><br>\n","\n","**Probability:**\n","The probability of each state to gain a reward of **1** when the episode ends at state **G**.\n","\n","State ${\\space}$Probability<br>\n","${\\space\\space}$A ${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.0<br>\n","${\\space\\space}$B${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.167<br>\n","${\\space\\space}$C${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.333<br>\n","${\\space\\space}$D${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.50<br>\n","${\\space\\space}$E${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.667<br>\n","${\\space\\space}$F${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$0.833<br>\n","${\\space\\space}$G${\\space\\space\\space\\space\\space\\space\\space\\space\\space\\space}$1.0<br>\n","\n","\n","    Note: At present, we know the environment and the target values.\n","          Let's use this information to implement TD (λ).\n","          Remember, state G is the most visited state as it has \n","          maximum value."]},{"cell_type":"markdown","metadata":{"id":"qPqrERpTL8in"},"source":["**Implementation ways:**\n","There are two ways of implementing TD ($λ$):\n","* **Forward view:** \n","\n","  Looking all n-steps ahead and updating the future estimates with the help of $λ$ that decay the estimates\n","\n","\n","         Note: In this example, we are considering backward view. \n","               Remember, the backward and forward view are equivalent.\n","\n","* **Backward view:**\n","\n","  In the episode, all the previous states get updated at each step. \n","  Now, the question that arises is how does one assign credits to the prior states. \n","\n","  **Eligibility traces (ET):**\n","  It keeps a track of the recency and frequency of entering a particular state and assign credits to these states with respect to the terminals state.<br><br>\n","\n","  Let us consider the state **F** in the environment that was described above.\n","\n","  ![RL L6 TD example 2](https://drive.google.com/uc?id=1Ep9o8ZqN50ir6uzrvX3Wdz6AEtZFN0le)\n","\n","  Now, let us see how ET works on state **F** and other states:\n","  * Value of State **F** will be updated multiple times\n","  * Hence, state **F** will be assigned more credits in proportion to TD error, whereas <br>the value of state **B** will not be updated frequently and will be closer to $0$ as it's visited<br>less frequently.\n","<br><br>\n","\n","**Algorithm for on-line TD:**<br><br>\n","![RL L6 TD 3](https://drive.google.com/uc?id=1hdieLLMXY1XDIRk4fAuyyMNfSTCn0aj2)"]},{"cell_type":"markdown","metadata":{"id":"WAyRmIo6YqVR"},"source":["#### **TD Control**"]},{"cell_type":"markdown","metadata":{"id":"i2TQRUaXvI-q"},"source":["##### **Types of TD Control** "]},{"cell_type":"markdown","metadata":{"id":"eL1AreN9vTbP"},"source":["TD prediction enables us to estimate the value function, whereas TD control <br>helps us optimize the value function. \n","\n","Two types of algorithms are used for TD control:\n","* Q-learning: Off-policy learning algorithm\n","* SARSA: On-policy learning algorithm "]},{"cell_type":"markdown","metadata":{"id":"CkEAaM9UB965"},"source":["##### **Q-learning**"]},{"cell_type":"markdown","metadata":{"id":"layO5U9ECBi0"},"source":["Q-Learning is a basic off-policy reinforcement learning algorithm which uses Q-values (or action values) to find the best action to take based on the current state to iteratively improvE the behavior of the learning agent. The **Q** in Q-learning represents **Quality** which determines the usefulness of  a given action in gaining some future rewards.\n","\n","It is considered as an off-policy algorithm due to the fact that this algorithm learns from the actions that are outside the current policy (like taking random actions). Due to this, a policy is not needed. It tries to learn a policy which maximizes the total reward.\n","\n","Below are some key points for Q-learning:\n","\n","* It is an off-policy TD control algorithm where an agent learns the policy π<br> from the experience sampled by a behavior policy µ.\n","\n","* Policies of interest in Q-learning are:\n","  * Target policy: It is a greedy policy\n","  * Behavior policy: It is a ε-greedy policy\n","\n","\n","* In Q-learning, the area of interest is **state-action pair**, where the Q-values <br> are updated using the state-action values of the successor states $s'$ and<br> greedy action $a'$."]},{"cell_type":"markdown","metadata":{"id":"jtbwa686Tqhu"},"source":["###### **Q-Learning Algorithm**"]},{"cell_type":"markdown","metadata":{"id":"bhqcjxU3OBmP"},"source":["![RL L6 Q-learning](https://drive.google.com/uc?id=1oNPE-N-W9r__X5u5MUnXOL8_hUwEgRAX)\n","\n","    Source: Reinforcement Learning by Richard S. Sutton\n","\n","    Note: Refer to the equation from the algorithm to describe the target and error."]},{"cell_type":"markdown","metadata":{"id":"26w6JAIsUTcT"},"source":["**Steps in Q-Learning:**\n","\n","1. Initialize the Q function\n","2. Take an action from a state by using an epsilon-greedy policy and move<br> to the successor state\n","3. Update the Q value of a previous state by following the update rule\n","4. Repeat steps 2 and 3 until the agent arrives at the terminal state"]},{"cell_type":"markdown","metadata":{"id":"zLtfaA0ZWUp9"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"78PsMPQRWaku"},"source":["**Problem Statement:** Solve the cliff walking environment using Q-learning."]},{"cell_type":"markdown","metadata":{"id":"2cn4rhqGVKqV"},"source":["##### **SARSA**"]},{"cell_type":"markdown","metadata":{"id":"l0a6tvSnnq49"},"source":["* Is an on-policy algorithm \n","* Learns the Q-value using the actions exhibited by the current policy"]},{"cell_type":"markdown","metadata":{"id":"QNBhmv1tpbMe"},"source":["**Sequence of Events in SARSA:**\n","\n","$S1 (current\\space state) → A1 (current\\space action) → R (current \\space reward) → S2 (next \\space state) → A2 (next \\space action)$ "]},{"cell_type":"markdown","metadata":{"id":"95zE8Yyzpoxv"},"source":["###### **SARSA Algorithm**"]},{"cell_type":"markdown","metadata":{"id":"Ira11xeQp50v"},"source":["![RL L6 SARSA](https://drive.google.com/uc?id=1pOTPPBBlislmYMiK7Vl2jWxscKIJSjWM)\n","\n","    Source: Reinforcement Learning by Richard S. Sutton\n","\n","    Note: Refer to the equation from the algorithm to describe the target and error."]},{"cell_type":"markdown","metadata":{"id":"BoiZEcn3prj9"},"source":["**Following are the hyperparameters involved:**\n","\n","* Learning rate (α)\n","* Discount factor (γ)\n","* Initial conditions i.e., Q (st,at)\n","\n","\n","    Note: These hyperparameters are discussed in TD prediction and the previous lesson, MDP."]},{"cell_type":"markdown","metadata":{"id":"1IdP_lzIWhkf"},"source":["##### **Assisted Practice**"]},{"cell_type":"markdown","metadata":{"id":"vSwwEidVWlCo"},"source":["**Problem Statement:** Solve the cliff walking environment using SARSA."]},{"cell_type":"markdown","metadata":{"id":"FSuviTvTl9DD"},"source":["## <b> Knowledge Check </b>"]},{"cell_type":"markdown","metadata":{"id":"eCTkCh0-l35y"},"source":["Click [here](https://www.dropbox.com/s/vi8ifec9h4inkhx/Lesson%206%20Knowledge%20Check.pptx?dl=0) to access knowledge check."]},{"cell_type":"markdown","metadata":{"id":"k6_96WSFGSgl"},"source":["## **Key Takeaways**"]},{"cell_type":"markdown","metadata":{"id":"OJbq6ow5GXs2"},"source":["* DP policy iteration always starts with a random baseline policy $π$ and improves it over time until it stabilizes and does not experience any changes in the newly derived policy $π^∗$.\n","\n","\n","* MC method is a model-free method that does not require prior information of the environment and transition probabilities for estimation.\n","\n","\n","* TD method is a combination of the properties of experiencing the environment (without the prior knowledge of the environment) and updating the value functions using the learned estimates."]},{"cell_type":"markdown","metadata":{"id":"Lo2oxovH5UVf"},"source":["![RL 1](https://drive.google.com/uc?id=17MoU7OAf-PPqV3cisfaCVaw6LYtAcrWm)"]}]}