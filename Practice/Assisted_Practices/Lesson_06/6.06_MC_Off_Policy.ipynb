{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ElkXWWcR1Loa"
   },
   "source": [
    "## **Problem Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q4Rz6J__1NlD"
   },
   "source": [
    "Train the agent to learn and win the card game, Blackjack using the off-policy MC method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylMuCnna1QhM"
   },
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "11t5mcJF1Sc0"
   },
   "source": [
    "* Game is played against a fixed dealer\n",
    "* Game has a replacement or an infinite deck\n",
    "* Moves:\n",
    "  * Hit = Player asking for additional card\n",
    "  * Stick = Player stops asking for the additional card\n",
    "  * Bust = The sum of all cards exceeds 21 \n",
    "* Score of the cards:\n",
    "  * Each of the cards Jack, Queen, and King have reward of 10\n",
    "  * Each ace has a reward of 11 or 1 and is called unstable at 11\n",
    "* Goal: Acquire cards that add up to 21 and must not go beyond 21.\n",
    "* Rules:\n",
    "  * Game starts with one card faced up and one card faced down for the player and the dealer\n",
    "  * Player can ask for additional cards until the sum of the cards exceed 21 or player stops voluntarily\n",
    "  * After the player sticks, the dealer will show the facedown card and draw cards from the deck until the sum is 17 or greater\n",
    "  * After drawing cards, the player wins if the dealer exceeds the allowed sum of 21 and vice versa\n",
    "  * If neither of them busts, the winner is decided by finding who has a score near to 21 \n",
    "* Action:\n",
    "  * STICK = 0\n",
    "  * HIT = 1\n",
    "* Reward:\n",
    "  * Win = +1\n",
    "  * Draw = 0\n",
    "  * Loss = -1\n",
    "* Observation:\n",
    "  * Current sum of players\n",
    "  * Dealer's one showing card\n",
    "  * Player having a usable ace or not\n",
    "\n",
    "Environment courtsey: This environment corresponds to the version of the Blackjack problem described in Example 5.1 in<br> Reinforcement Learning: An Introduction by Sutton and Barto (1998), and OpenAI Gym."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "monlIs_P1mK8"
   },
   "source": [
    "## **Import the libraries and environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1593791188974,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "YGMRFEtL1KN_"
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import numpy as np\n",
    "import sys\n",
    "import collections\n",
    "from collections import defaultdict\n",
    "from matplotlib import pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import gym\n",
    "env = gym.make(\"Blackjack-v1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fw6JCl681y2v"
   },
   "source": [
    "## **Solution**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pzBl4nVx6rrK"
   },
   "source": [
    "**Arguments:**\n",
    "\n",
    "* policy: Maps an observation to action probabilities\n",
    "* env: OpenAI Gym environment\n",
    "* num_episodes: Number of episodes\n",
    "* discount_factor: Gamma discount factor\n",
    "* Q: A dictionary that maps from state -> action-values. Each value is a numpy array of length nA (see below)\n",
    "* epsilon: Probability to select a random action float between 0 and 1\n",
    "* nA: Number of actions in the environment\n",
    "* Returns:\n",
    "  * A = Function that takes the observation as an argument and returns the probabilities for each action in the form of a numpy array of length nA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zbhCSKmD1KOO"
   },
   "source": [
    "### **Monte Carlo Off-Policy Control**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1190,
     "status": "ok",
     "timestamp": 1593791188975,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "zISW8nu-1KOP"
   },
   "outputs": [],
   "source": [
    "#Creating a random policy\n",
    "def create_random_policy(nA):\n",
    "\n",
    "    A = np.ones(nA, dtype=float) / nA\n",
    "    def policy_fn(observation):\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1187,
     "status": "ok",
     "timestamp": 1593791188975,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "NBWsgwKJ9_rg"
   },
   "outputs": [],
   "source": [
    "#Arguments:\n",
    "#Q: A dictionary that maps from state-action values\n",
    "#Returns: A function that returns a vector of action probabilities\n",
    "def create_greedy_policy(Q):\n",
    "    \n",
    "    def policy_fn(state):\n",
    "        A = np.zeros_like(Q[state], dtype=float)\n",
    "        best_action = np.argmax(Q[state])\n",
    "        A[best_action] = 1.0\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1730,
     "status": "ok",
     "timestamp": 1593791189522,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "KxzguI-W1KOT"
   },
   "outputs": [],
   "source": [
    "#Monte Carlo Control: It is an off-policy control method that finds an optimal greedy policy.\n",
    "#Arguments:\n",
    "#env: Blackjack environment.\n",
    "#num_episodes: Number of episodes for a sample\n",
    "#behavior_policy: Following a behavior while generating episodes\n",
    "#discount_factor: Gamma\n",
    "#Returns:\n",
    "#A = Tuple of Q and policy\n",
    "#Q = Dictionary mapping state-action values.\n",
    "#policy = Function that returns action probabilities. This is the optimal greedy policy.\n",
    "\n",
    "def mc_control_importance_sampling(env, num_episodes, behavior_policy, discount_factor=1.0):\n",
    "    \n",
    "    \n",
    "    # Final action-value function.\n",
    "    # A dictionary that maps state-action values\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    # The cumulative denominator of the weighted importance sampling formula (across all episodes)\n",
    "    C = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "    \n",
    "    # Our greedily policy we want to learn\n",
    "    target_policy = create_greedy_policy(Q)\n",
    "        \n",
    "    for i_episode in range(1, num_episodes + 1):\n",
    "        # Print out which episode we're on, useful for debugging.\n",
    "        if i_episode % 1000 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "\n",
    "        # Generate an episode.\n",
    "        # An episode is an array of (state, action, reward) tuples\n",
    "        episode = []\n",
    "        state = env.reset()\n",
    "        for t in range(100):\n",
    "            # Sample an action from our policy\n",
    "            probs = behavior_policy(state)\n",
    "            action = np.random.choice(np.arange(len(probs)), p=probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            episode.append((state, action, reward))\n",
    "            if done:\n",
    "                break\n",
    "            state = next_state\n",
    "        \n",
    "        # Sum of discounted returns\n",
    "        G = 0.0\n",
    "        # The importance sampling ratio (the weights of the returns)\n",
    "        W = 1.0\n",
    "        # For each step in the episode, backwards\n",
    "        for t in range(len(episode))[::-1]:\n",
    "            state, action, reward = episode[t]\n",
    "            # Update the total reward since step t\n",
    "            G = discount_factor * G + reward\n",
    "            # Update weighted importance sampling formula denominator\n",
    "            C[state][action] += W\n",
    "            # Update the action-value function using the incremental update formula (5.7)\n",
    "            # This also improves our target policy which holds a reference to Q\n",
    "            Q[state][action] += (W / C[state][action]) * (G - Q[state][action])\n",
    "            # If the action taken by the behavior policy is not the action \n",
    "            # taken by the target policy the probability will be 0 and we can break\n",
    "            if action !=  np.argmax(target_policy(state)):\n",
    "                break\n",
    "            W = W * 1./behavior_policy(state)[action]\n",
    "        \n",
    "    return Q, target_policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "35weqfQk2jIi"
   },
   "source": [
    "#### **Episodes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "35d3c8pe1KOV",
    "outputId": "300e2517-1e68-44f3-d996-e18491714d5b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500000/500000."
     ]
    }
   ],
   "source": [
    "random_policy = create_random_policy(env.action_space.n)\n",
    "Q, policy = mc_control_importance_sampling(env, num_episodes=500000, behavior_policy=random_policy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q_699otl1KOZ"
   },
   "source": [
    "### **Create Plot Function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSYU-AeX1KOZ"
   },
   "outputs": [],
   "source": [
    "def plot_value_function(V, title=\"Value Function\"):\n",
    "    \n",
    "    min_x = min(k[0] for k in V.keys())\n",
    "    max_x = max(k[0] for k in V.keys())\n",
    "    min_y = min(k[1] for k in V.keys())\n",
    "    max_y = max(k[1] for k in V.keys())\n",
    "\n",
    "    x_range = np.arange(min_x, max_x + 1)\n",
    "    y_range = np.arange(min_y, max_y + 1)\n",
    "    X, Y = np.meshgrid(x_range, y_range)\n",
    "\n",
    "    # Find value for all (x, y) coordinates\n",
    "    Z_noace = np.apply_along_axis(lambda _: V[(_[0], _[1], False)], 2, np.dstack([X, Y]))\n",
    "    Z_ace = np.apply_along_axis(lambda _: V[(_[0], _[1], True)], 2, np.dstack([X, Y]))\n",
    "\n",
    "    def plot_surface(X, Y, Z, title):\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        ax = fig.add_subplot(111, projection='3d')\n",
    "        surf = ax.plot_surface(X, Y, Z, rstride=1, cstride=1,\n",
    "                               cmap=matplotlib.cm.coolwarm, vmin=-1.0, vmax=1.0)\n",
    "        ax.set_xlabel('Player Sum')\n",
    "        ax.set_ylabel('Dealer Showing')\n",
    "        ax.set_zlabel('Value')\n",
    "        ax.set_title(title)\n",
    "        ax.view_init(ax.elev, -120)\n",
    "        fig.colorbar(surf)\n",
    "        plt.show()\n",
    "\n",
    "    plot_surface(X, Y, Z_noace, \"{} (No Usable Ace)\".format(title))\n",
    "    plot_surface(X, Y, Z_ace, \"{} (Usable Ace)\".format(title))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vCe87Lye2vFh"
   },
   "source": [
    "#### **Plot the Control**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qJ9uKaLL1KOh"
   },
   "outputs": [],
   "source": [
    "# For plotting: Create value function from action-value function\n",
    "# by picking the best action at each state\n",
    "V = defaultdict(float)\n",
    "for state, action_values in Q.items():\n",
    "    action_value = np.max(action_values)\n",
    "    V[state] = action_value\n",
    "plot_value_function(V, title=\"Optimal Value Function\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_6_MC_Off_Policy.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
