{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ah2wqyXoo4Wa"
   },
   "source": [
    "# **Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOM0CbF0o8NO"
   },
   "source": [
    "## **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_sB7g9JpArP"
   },
   "source": [
    "Solve the cliff walking environment using Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyrzFiXEpKeF"
   },
   "source": [
    "### **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grS0W4WCpMWa"
   },
   "source": [
    "Cliff walking is a two dimensional board game.\n",
    "\n",
    "![Demo 7 Q-learning](https://drive.google.com/uc?id=1rLg6GH1ErppUumPO06NTEMROQjwlMHxw)\n",
    "\n",
    "* The environment has episodic tasks.\n",
    "\n",
    "* Agent starts at the state **S** and traverses through the environment to reach goal **G**.\n",
    "\n",
    "* Agent can move up, down, right, and left.\n",
    "\n",
    "* Agent receives a reward of -1 for every movement except for the cliff and the goal.\n",
    "\n",
    "* Agent falling into the cliff attracts a reward of -100 and is sent back to the state **S**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UidA-Rp0vXZm"
   },
   "source": [
    "### **Import tools and environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VtXrlIQTnLyO"
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'discrete' from 'gym.envs.toy_text' (/usr/local/lib/python3.10/site-packages/gym/envs/toy_text/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_302/2031611399.py\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mcliff_walking\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnamedtuple\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/RL/0.4 Course Components/0.1 Assisted Practices/Lesson 6/Demo 7/cliff_walking.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoy_text\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdiscrete\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'discrete' from 'gym.envs.toy_text' (/usr/local/lib/python3.10/site-packages/gym/envs/toy_text/__init__.py)"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import itertools\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import cliff_walking\n",
    "from collections import namedtuple\n",
    "from matplotlib import pyplot as plt\n",
    "from collections import defaultdict\n",
    "from cliff_walking import CliffWalkingEnv\n",
    "matplotlib.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "At73wIYVnQtc"
   },
   "outputs": [],
   "source": [
    "env = CliffWalkingEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HG8YN8pQvhjl"
   },
   "source": [
    "### **Create the $\\epsilon-greedy$ policy** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E_PDb8V8nTu0"
   },
   "outputs": [],
   "source": [
    "#Creating an epsilon-greedy policy using Q-function and epsilon.\n",
    "#Arguments:\n",
    "#Q: Action-values\n",
    "#epsilon: Probability of selecting a random action\n",
    "#nA: Number of actions available in the environment\n",
    "#Returns: Probabilities for each action will be presented in the form of a numpy array of length nA\n",
    "\n",
    "\n",
    "def make_epsilon_greedy_policy(Q, epsilon, nA):\n",
    "   \n",
    "    def policy_fn(observation):\n",
    "        A = np.ones(nA, dtype=float) * epsilon / nA\n",
    "        best_action = np.argmax(Q[observation])\n",
    "        A[best_action] += (1.0 - epsilon)\n",
    "        return A\n",
    "    return policy_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YBatYrYYvypJ"
   },
   "source": [
    "### **Q-Learning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOJh6EQCnYEK"
   },
   "outputs": [],
   "source": [
    "#Q-Learning algorithm: Its an off-policy TD control used to find the optimal greedy policy\n",
    "#It follows an epsilon-greedy policy\n",
    "#Arguments:\n",
    "#env: Cliff walking environment\n",
    "#num_episodes: Number of episodes\n",
    "#discount_factor: Gamma\n",
    "#alpha: learning rate.\n",
    "#epsilon: Probability of sampling a random action   \n",
    "#Returns:\n",
    "#A = It is a tuple of Q, episode_lengths\n",
    "#Q = It is the optimal action-value function\n",
    "#Stats = It is an EpisodeStats object with two numpy arrays for episode_lengths and episode_rewards\n",
    "\n",
    "\n",
    "def q_learning(env, num_episodes, discount_factor=1.0, alpha=0.5, epsilon=0.1):\n",
    "   \n",
    "    \n",
    "    #The final action-value function.\n",
    "    #A nested dictionary that maps state to (action -> action-value)\n",
    "    Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "    #Tracking useful statistics\n",
    "    EpisodeStats = namedtuple(\"Stats\",[\"episode_lengths\", \"episode_rewards\"]) #storing stats in tuples helps during plotting the stats\n",
    "\n",
    "    \n",
    "    stats = EpisodeStats(\n",
    "        episode_lengths=np.zeros(num_episodes),\n",
    "        episode_rewards=np.zeros(num_episodes))    \n",
    "    \n",
    "    #Policy followed by us\n",
    "    policy = make_epsilon_greedy_policy(Q, epsilon, env.action_space.n)\n",
    "    \n",
    "    for i_episode in range(num_episodes):\n",
    "        # Printing out which episode we're on (useful for debugging)\n",
    "        if (i_episode + 1) % 100 == 0:\n",
    "            print(\"\\rEpisode {}/{}.\".format(i_episode + 1, num_episodes), end=\"\")\n",
    "            sys.stdout.flush()\n",
    "        \n",
    "        # Reseting the environment and picking up the first action\n",
    "        state = env.reset()\n",
    "        \n",
    "        # One step in the environment\n",
    "        # total_reward = 0.0\n",
    "        for t in itertools.count():\n",
    "            \n",
    "            # Take a step\n",
    "            action_probs = policy(state)\n",
    "            action = np.random.choice(np.arange(len(action_probs)), p=action_probs)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "\n",
    "            # Update statistics\n",
    "            stats.episode_rewards[i_episode] += reward\n",
    "            stats.episode_lengths[i_episode] = t\n",
    "            \n",
    "            # TD Update\n",
    "            best_next_action = np.argmax(Q[next_state])    \n",
    "            td_target = reward + discount_factor * Q[next_state][best_next_action]\n",
    "            td_delta = td_target - Q[state][action]\n",
    "            Q[state][action] += alpha * td_delta\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "                \n",
    "            state = next_state\n",
    "    \n",
    "    return Q, stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rJAkieF9naHq"
   },
   "outputs": [],
   "source": [
    "Q, stats = q_learning(env, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dxJhOw0Pv3DS"
   },
   "source": [
    "### **Plot the stats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KZ1lLOALnflt"
   },
   "outputs": [],
   "source": [
    "def plot_episode_stats(stats, smoothing_window=10, noshow=False):\n",
    "    # Plot the episode length over time\n",
    "    fig1 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(stats.episode_lengths)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length over Time\")\n",
    "    if noshow:\n",
    "        plt.close(fig1)\n",
    "    else:\n",
    "        plt.show(fig1)\n",
    "\n",
    "    # Plot the episode reward over time\n",
    "    fig2 = plt.figure(figsize=(10,5))\n",
    "    rewards_smoothed = pd.Series(stats.episode_rewards).rolling(smoothing_window, min_periods=smoothing_window).mean()\n",
    "    plt.plot(rewards_smoothed)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Reward (Smoothed)\")\n",
    "    plt.title(\"Episode Reward over Time (Smoothed over window size {})\".format(smoothing_window))\n",
    "    if noshow:\n",
    "        plt.close(fig2)\n",
    "    else:\n",
    "        plt.show(fig2)\n",
    "\n",
    "    # Plot time steps and episode number\n",
    "    fig3 = plt.figure(figsize=(10,5))\n",
    "    plt.plot(np.cumsum(stats.episode_lengths), np.arange(len(stats.episode_lengths)))\n",
    "    plt.xlabel(\"Time Steps\")\n",
    "    plt.ylabel(\"Episode\")\n",
    "    plt.title(\"Episode per time step\")\n",
    "    if noshow:\n",
    "        plt.close(fig3)\n",
    "    else:\n",
    "        plt.show(fig3)\n",
    "\n",
    "    return fig1, fig2, fig3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3167,
     "status": "ok",
     "timestamp": 1592466549285,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "mh8A9PtUncGs",
    "outputId": "3aea4828-430c-475c-bee3-de2bda4a687e"
   },
   "outputs": [],
   "source": [
    "plot_episode_stats(stats)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_7_Q_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
