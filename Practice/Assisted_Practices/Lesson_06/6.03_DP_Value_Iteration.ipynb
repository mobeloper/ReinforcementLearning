{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8OoRAiFBs-l"
   },
   "source": [
    "# **Value Iteration in GridWorld**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLWRKYoEEDV0"
   },
   "source": [
    "## **Problem Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfcJfykDIyiD"
   },
   "source": [
    "Company Robo.ai is building a robot that can traverse unassisted, through the environment, and reach the food counter. Instead of creating their own environment, they have planned to use a prebuilt 4x4 grid world. You are a researcher who has to identify the policy and value iteration methods to tackle this task. You have decided to go with the value iteration method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQFsPa_-LUm-"
   },
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNRT5rmREJfl"
   },
   "source": [
    "This environment possesses two terminal states present at:<br>\n",
    "* Top left corner\n",
    "* Bottom right corner\n",
    "\n",
    "<br>\n",
    "The 4x4 grid looks as follows:<br>\n",
    "T  o  o  o<br>\n",
    "o  x  o  o<br>\n",
    "o  o  o  o<br>\n",
    "o  o  o  T<br>\n",
    "Where x is the position of the agent and T are the two terminal states.<br>\n",
    "\n",
    "<br>\n",
    "The actions allowed are as follows:\n",
    "* UP = 0 \n",
    "* RIGHT = 1 \n",
    "* DOWN = 2 \n",
    "* LEFT = 3 <br>\n",
    "\n",
    "\n",
    "    Note: The agent will move back to current states if it performs an action that leads it to go off the edge.\n",
    "\n",
    "Rewards:\n",
    "The agent is granted a reward of -1 at each step until it reaches a terminal state.\n",
    "\n",
    "Environment courtesy: Sutton's Reinforcement Learning book, chapter 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KzfC5lALiH7"
   },
   "source": [
    "### **Dependencies**\n",
    "* Discrete\n",
    "* Gridworld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krb3JtvVLYF9"
   },
   "source": [
    "## **Import libraries and environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oE9AfY72CI5E"
   },
   "outputs": [],
   "source": [
    "import numpy as nump\n",
    "import sys\n",
    "from gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS5CVTSwCI5J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "environment = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfyylf5PLyLH"
   },
   "source": [
    "## **Value Iteration**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOCx4_o1P3wq"
   },
   "source": [
    "Arguments:\n",
    "    \n",
    "* policy = [S, A] shaped matrix\n",
    "* environment.P = Transition probabilities\n",
    "* environment.P[s][a] = Transition tuple (prob, next_state, reward, done)\n",
    "* environment.nS = Number of states \n",
    "* environment.nA = Number of actions\n",
    "* theta = Stopping the evaluation once the value function changes is less than theta for all the states\n",
    "* discount_factor = Gamma discount factor\n",
    "* Returns = Tuple of optimal policy and value function\n",
    "* Returns = Vector of length of action in the environment (contains the value of each action)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adqFxE_wcuzy"
   },
   "outputs": [],
   "source": [
    "#Defining Value iteration as environment, discount factor, and theta \n",
    "def value_iteration(environment, theta=0.0001, discount_factor=1.0):\n",
    "  \n",
    "    #Defining one-step lookahead to calculate the value function of current state\n",
    "    def one_step_lookahead(state, Val_function):\n",
    "        \n",
    "        A = nump.zeros(environment.nA)\n",
    "        for a in range(environment.nA):\n",
    "            for prob, next_state, reward, done in environment.P[state][a]:\n",
    "                A[a] += prob * (reward + discount_factor * Val_function[next_state])\n",
    "        return A\n",
    "\n",
    "    Val_function = nump.zeros(environment.nS)\n",
    "    while True:\n",
    "        # Stopping condition\n",
    "        delta = 0\n",
    "        # Update each state...\n",
    "        for s in range(environment.nS):\n",
    "            # Finding the best action with one-step lookahead\n",
    "            A = one_step_lookahead(s, Val_function)\n",
    "            best_action_value = nump.max(A)\n",
    "            # Delta for all states that have been observed so far\n",
    "            delta = max(delta, nump.abs(best_action_value - Val_function[s]))\n",
    "            # Value function update\n",
    "            Val_function[s] = best_action_value        \n",
    "        # Stopping the iteration if exceeded the threshold \n",
    "        if delta < theta:\n",
    "            break\n",
    "    \n",
    "    # Using the optimal value function for creating a deterministic policy\n",
    "    policy = nump.zeros([environment.nS, environment.nA])\n",
    "    for s in range(environment.nS):\n",
    "        # Finding the best action for current state using one-step lookahead\n",
    "        A = one_step_lookahead(s, Val_function)\n",
    "        best_action = nump.argmax(A)\n",
    "        # Taking the best action\n",
    "        policy[s, best_action] = 1.0\n",
    "    \n",
    "    return policy, Val_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 603
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 840,
     "status": "ok",
     "timestamp": 1593790718993,
     "user": {
      "displayName": "Ajay Pal Singh",
      "photoUrl": "",
      "userId": "06233761347829400527"
     },
     "user_tz": -330
    },
    "id": "DdYav1L5ISra",
    "outputId": "f275d251-ecc4-4db5-f60c-28f18c0dbd45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Probability Distribution:\n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\n",
      "[[0 3 3 2]\n",
      " [0 0 0 2]\n",
      " [0 0 1 2]\n",
      " [0 1 1 0]]\n",
      "\n",
      "Value Function:\n",
      "[ 0. -1. -2. -3. -1. -2. -3. -2. -2. -3. -2. -1. -3. -2. -1.  0.]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "policy, Val_function = value_iteration(environment)\n",
    "\n",
    "print(\"Policy Probability Distribution:\")\n",
    "print(policy)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Policy (0=up, 1=right, 2=down, 3=left):\")\n",
    "print(nump.reshape(nump.argmax(policy, axis=1), environment.shape))\n",
    "print(\"\")\n",
    "\n",
    "print(\"Value Function:\")\n",
    "print(Val_function)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(Val_function.reshape(environment.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_3_DP_Value_Iteration.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
