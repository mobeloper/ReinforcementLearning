{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J8OoRAiFBs-l"
   },
   "source": [
    "# **Policy Evaluation in GridWorld**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LLWRKYoEEDV0"
   },
   "source": [
    "## **Problem Statement:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RfcJfykDIyiD"
   },
   "source": [
    "Implement policy evaluation method on GridWorld."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oQFsPa_-LUm-"
   },
   "source": [
    "## **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dNRT5rmREJfl"
   },
   "source": [
    "* This environment possesses two terminal states present at:<br>\n",
    "  * Top left corner\n",
    "  * Bottom right corner\n",
    "<br>\n",
    "The 4x4 grid looks as follows:<br>\n",
    "T  o  o  o<br>\n",
    "o  x  o  o<br>\n",
    "o  o  o  o<br>\n",
    "o  o  o  T<br>\n",
    "\n",
    "    Where **x** is the position of the agent and **T** are the two terminal states.<br>\n",
    "\n",
    "* The allowed actions are as follows:\n",
    "  * UP = 0 \n",
    "  * RIGHT = 1 \n",
    "  * DOWN = 2 \n",
    "  * LEFT = 3 <br>\n",
    "\n",
    "\n",
    "    Note: The agent will move back to current states if it performs an action that leads it to go off the edge.\n",
    "\n",
    "* Rewards:<br>The agent is granted a reward of -1 at each step until it reaches a terminal state.\n",
    "\n",
    "    \n",
    "    Environment courtesy: Sutton's Reinforcement Learning book, chapter 4.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5KzfC5lALiH7"
   },
   "source": [
    "### **Dependencies**\n",
    "* [Discrete](https://drive.google.com/file/d/1aLV-ln3qZgDQbbGZVDQaez4buV9EhdwJ/view?usp=sharing)\n",
    "* [Gridworld](https://drive.google.com/file/d/1MdOVjmYzSR4Gg1AX3AnwnpXqCMfMPcax/view?usp=sharing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "krb3JtvVLYF9"
   },
   "source": [
    "## **Import libraries and environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_lN7iJiAStVE"
   },
   "source": [
    "    Note: Put discrete and gridworld in worknig directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oE9AfY72CI5E"
   },
   "outputs": [],
   "source": [
    "import numpy as nump\n",
    "import sys\n",
    "from gridworld import GridworldEnv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rS5CVTSwCI5J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/gym/utils/seeding.py:41: DeprecationWarning: \u001b[33mWARN: Function `rng.rand(*size)` is marked as deprecated and will be removed in the future. Please use `Generator.random(size)` instead.\u001b[0m\n",
      "  deprecation(\n"
     ]
    }
   ],
   "source": [
    "environment = GridworldEnv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mfyylf5PLyLH"
   },
   "source": [
    "## **Evaluate the policy**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VOCx4_o1P3wq"
   },
   "source": [
    "Arguments:\n",
    "    \n",
    "* policy = [S, A] shaped matrix\n",
    "* environment.P = Transition probabilities\n",
    "* environment.P[s][a] = Transition tuple (prob, next_state, reward, done)\n",
    "* environment.nS = Number of states \n",
    "* environment.nA = Number of actions\n",
    "* theta = Stopping the evaluation once the value function change is less than theta for all the states\n",
    "* discount_factor = Gamma discount factor\n",
    "* Returns = Value function in form of a vector of length environment.nS\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "adqFxE_wcuzy"
   },
   "outputs": [],
   "source": [
    "def policy_eval(policy, environment, discount_factor=1.0, theta=0.00001):\n",
    "    \n",
    "    # Start with a random value function where the value is 0 for all the states.\n",
    "    Val_function = nump.zeros(environment.nS)\n",
    "    while True:\n",
    "      \n",
    "        delta = 0\n",
    "        # Perform a \"full backup\" for each state\n",
    "        for s in range(environment.nS):\n",
    "            v = 0\n",
    "            \n",
    "            # Look at all the possible next actions\n",
    "            for a, action_prob in enumerate(policy[s]):\n",
    "              \n",
    "                # Look at the possible next states in accordance to all the 4 types of actions\n",
    "                for  prob, next_state, reward, done in environment.P[s][a]:\n",
    "                  \n",
    "                    # Calculate the expected value\n",
    "                    v += action_prob * prob * (reward + discount_factor * Val_function[next_state])\n",
    "                    \n",
    "            # Register the change in value function across any state\n",
    "            delta = max(delta, nump.abs(v - Val_function[s]))\n",
    "            Val_function[s] = v\n",
    "              \n",
    "        # Cease the evaluation once the value function change is below a threshold i.e, theta\n",
    "        if delta < theta:\n",
    "            break\n",
    "    return nump.array(Val_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OvGqToH_cu4v"
   },
   "outputs": [],
   "source": [
    "Random_policy = nump.ones([environment.nS, environment.nA]) / environment.nA\n",
    "v = policy_eval(Random_policy, environment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9nQ1Zhx9MuEh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function:\n",
      "[  0.         -13.99993529 -19.99990698 -21.99989761 -13.99993529\n",
      " -17.9999206  -19.99991379 -19.99991477 -19.99990698 -19.99991379\n",
      " -17.99992725 -13.99994569 -21.99989761 -19.99991477 -13.99994569\n",
      "   0.        ]\n",
      "\n",
      "Reshaped Grid Value Function:\n",
      "[[  0.         -13.99993529 -19.99990698 -21.99989761]\n",
      " [-13.99993529 -17.9999206  -19.99991379 -19.99991477]\n",
      " [-19.99990698 -19.99991379 -17.99992725 -13.99994569]\n",
      " [-21.99989761 -19.99991477 -13.99994569   0.        ]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Value Function:\")\n",
    "print(v)\n",
    "print(\"\")\n",
    "\n",
    "print(\"Reshaped Grid Value Function:\")\n",
    "print(v.reshape(environment.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ggkOO95NdDMY"
   },
   "source": [
    "## **Test the Evaluated Policy Against the Expected** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-RlN4mPIcvDF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match\n"
     ]
    }
   ],
   "source": [
    "expected_v = nump.array([0, -14, -20, -22, -14, -18, -20, -20, -20, -20, -18, -14, -22, -20, -14, 0])\n",
    "result = nump.testing.assert_array_almost_equal(v, expected_v, decimal=2)\n",
    "if result==None:\n",
    "    print(\"Match\")\n",
    "else:\n",
    "    print(\"No Match\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Demo_1_DP_Policy_Evaluation.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
