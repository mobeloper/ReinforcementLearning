{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ah2wqyXoo4Wa"
   },
   "source": [
    "# **Solve the CartPole Environment Problem Using Q-Learning**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOM0CbF0o8NO"
   },
   "source": [
    "## **Problem Statement**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_sB7g9JpArP"
   },
   "source": [
    "Solve the control problem in the CartPole environment using the Q-learning method. While solving the CartPole environment, run it for 500 episodes (i.e., epochs), where the epsilon is a fraction of 1/sqrt(n+1) and the discount factor is 0.99. Also, plot all the rewards and their running averages for all the episodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XyrzFiXEpKeF"
   },
   "source": [
    "### **Environment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "grS0W4WCpMWa"
   },
   "source": [
    "It is a CartPole environment where the task is to train the agent to keep the pole upright for as long as possible.\n",
    "\n",
    "Environment looks like this:\n",
    "![CartPole](https://drive.google.com/uc?id=1GGcPDgYpAuNRGdUtZprNu1AZaT2VFP30)\n",
    "\n",
    "**States in the environment are:**\n",
    "* Position of the cart (can be random)\n",
    "* Velocity of the cart\n",
    "* Angle of the pole\n",
    "* Angular velocity of the pole\n",
    "    \n",
    "\n",
    "**Actions performed by the agent:**<br>\n",
    "* Move the cart left\n",
    "* Move the cart right\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UidA-Rp0vXZm"
   },
   "source": [
    "## **Solution:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TqLRbRjhMzst"
   },
   "source": [
    "### **Import Libraries and Environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "At73wIYVnQtc"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_approximation import RBFSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aRw1brPcrOzn"
   },
   "source": [
    "## **Preprocessing Steps**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kGU1bwvcTTAZ"
   },
   "outputs": [],
   "source": [
    "# Minimizing a regularized empirical loss, pre-processing, and optimizing the environment using sklearn.\n",
    "class SGDRegressor:\n",
    "  def __init__(self, D):\n",
    "    self.w = np.random.randn(D) / np.sqrt(D)\n",
    "    self.lr = 0.1\n",
    "\n",
    "  def partial_fit(self, X, Y):\n",
    "    self.w += self.lr*(Y - X.dot(self.w)).dot(X)\n",
    "\n",
    "  def predict(self, X):\n",
    "    return X.dot(self.w)\n",
    "\n",
    "\n",
    "class FeatureTransformer:\n",
    "  def __init__(self, env):\n",
    "  \n",
    "    observation_examples = np.random.random((20000, 4))*2 - 1\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(observation_examples)\n",
    "\n",
    "    # Converting a state to a feature represenation by using RBF kernels with different variances to cover different parts of the space.\n",
    "    featurizer = FeatureUnion([\n",
    "            (\"rbf1\", RBFSampler(gamma=0.05, n_components=1000)),\n",
    "            (\"rbf2\", RBFSampler(gamma=1.0, n_components=1000)),\n",
    "            (\"rbf3\", RBFSampler(gamma=0.5, n_components=1000)),\n",
    "            (\"rbf4\", RBFSampler(gamma=0.1, n_components=1000))\n",
    "            ])\n",
    "    feature_examples = featurizer.fit_transform(scaler.transform(observation_examples))\n",
    "\n",
    "    self.dimensions = feature_examples.shape[1]\n",
    "    self.scaler = scaler\n",
    "    self.featurizer = featurizer\n",
    "\n",
    "  def transform(self, observations):\n",
    "    scaled = self.scaler.transform(observations)\n",
    "    return self.featurizer.transform(scaled)\n",
    "\n",
    "\n",
    "# Holding one SGDRegressor for each action\n",
    "class Model:\n",
    "  def __init__(self, env, feature_transformer):\n",
    "    self.env = env\n",
    "    self.models = []\n",
    "    self.feature_transformer = feature_transformer\n",
    "    for i in range(env.action_space.n):\n",
    "      model = SGDRegressor(feature_transformer.dimensions)\n",
    "      self.models.append(model)\n",
    "\n",
    "  def predict(self, s):\n",
    "    X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "    result = np.stack([m.predict(X) for m in self.models]).T\n",
    "    return result\n",
    "\n",
    "  def update(self, s, a, G):\n",
    "    X = self.feature_transformer.transform(np.atleast_2d(s))\n",
    "    self.models[a].partial_fit(X, [G])\n",
    "\n",
    "  def sample_action(self, s, eps):\n",
    "    if np.random.random() < eps:\n",
    "      return self.env.action_space.sample()\n",
    "    else:\n",
    "      return np.argmax(self.predict(s))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9qj2CtFG8cNb"
   },
   "source": [
    "## **Find total rewards**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "auU6Qw4Tbu6A"
   },
   "outputs": [],
   "source": [
    "def play_one(env, model, eps, gamma):\n",
    "  observation = env.reset()\n",
    "  done = False\n",
    "  totalreward = 0\n",
    "  iters = 0\n",
    "  while not done and iters < 2000:\n",
    "    # Quiting after reaching 2000 steps \n",
    "    action = model.sample_action(observation, eps)\n",
    "    prev_observation = observation\n",
    "    observation, reward, done, info = env.step(action)\n",
    "\n",
    "    if done:\n",
    "      reward = -200\n",
    "\n",
    "    # Updating the model\n",
    "    next = model.predict(observation)\n",
    "    assert(next.shape == (1, env.action_space.n))\n",
    "    G = reward + gamma*np.max(next)\n",
    "    model.update(prev_observation, action, G)\n",
    "\n",
    "    if reward == 1: # if we changed the reward to -200\n",
    "      totalreward += reward\n",
    "    iters += 1\n",
    "\n",
    "  return totalreward\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_I8wnArMGrZJ"
   },
   "source": [
    "## **Functions to Plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y5rrX1LwbzEK"
   },
   "outputs": [],
   "source": [
    "#Plot function\n",
    "def plot_running_avg(totalrewards):\n",
    "  N = len(totalrewards)\n",
    "  running_avg = np.empty(N)\n",
    "  for t in range(N):\n",
    "    running_avg[t] = totalrewards[max(0, t-100):(t+1)].mean()\n",
    "  plt.plot(running_avg)\n",
    "  plt.title(\"Running Average\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "remDCnYCHm64"
   },
   "source": [
    "## **Apply model on the environment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5Aukqs55bU5q"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:97: UserWarning: \u001b[33mWARN: We recommend you to use a symmetric and normalized Box action space (range=[-1, 1]) https://stable-baselines3.readthedocs.io/en/master/guide/rl_tips.html\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 total reward: 10.0 eps: 1.0 avg reward (last 100): 10.0\n",
      "episode: 100 total reward: 180.0 eps: 0.09950371902099892 avg reward (last 100): 121.7128712871287\n"
     ]
    }
   ],
   "source": [
    "#Running the environment for 500 episodes and where epsilon is a fraction of 1/sqrt(n+1) and discount factor is 0.99\n",
    "def main():\n",
    "  env = gym.make('CartPole-v1')\n",
    "  ft = FeatureTransformer(env)\n",
    "  model = Model(env, ft)\n",
    "  gamma = 0.99\n",
    "\n",
    "  N = 500\n",
    "  totalrewards = np.empty(N)\n",
    "  costs = np.empty(N)\n",
    "  for n in range(N):\n",
    "    eps = 1.0/np.sqrt(n+1)\n",
    "    totalreward = play_one(env, model, eps, gamma)\n",
    "    totalrewards[n] = totalreward\n",
    "    if n % 100 == 0:\n",
    "      print(\"episode:\", n, \"total reward:\", totalreward, \"eps:\", eps, \"avg reward (last 100):\", totalrewards[max(0, n-100):(n+1)].mean())\n",
    "\n",
    "  print(\"avg reward for last 100 episodes:\", totalrewards[-100:].mean())\n",
    "  print(\"total steps:\", totalrewards.sum())\n",
    "\n",
    "  plt.plot(totalrewards)\n",
    "  plt.title(\"Rewards\")\n",
    "  plt.show()\n",
    "\n",
    "  plot_running_avg(totalrewards)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "  main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Cartpole_Q_Learning.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 [3.10]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
